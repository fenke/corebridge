{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RScript-Bridge\n",
    "\n",
    "> Bridge between Stactics AICore framework and RScript prediction scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some things to set up first\n",
    "\n",
    "Notebooks use nbdev thingses and `addroot` makes importing from\n",
    "the repo-directory more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import logging\n",
    "#import addroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:corebridge.core:Loading corebridge.core from /home/fenke/repos/corebridge/corebridge/core.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corebridge.aicorebridge 0.2.23 from /home/fenke/repos/corebridge/corebridge/aicorebridge.py\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import os, logging, json, hashlib\n",
    "import fcntl, subprocess\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from corebridge.aicorebridge import AICoreModule\n",
    "from corebridge.core import init_console_logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AICore uses an `assets` dir from which we can read files, like scripts\n",
    "and a `save` dir were modules can write and read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "syslog = init_console_logging(__name__, logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assets_dir = os.path.join(os.path.abspath(os.getcwd()), 'assets', 'rscript')\n",
    "save_dir = os.path.join(os.path.abspath(os.getcwd()), 'saves', 'rscript')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def get_asset_path(script_name, assets_dir:str): \n",
    "    return os.path.join(assets_dir, script_name)\n",
    "def get_rscript_libpath(save_dir:str):\n",
    "    return os.path.join(save_dir, 'libs')\n",
    "def get_save_path(datafile_name:str, save_dir:str): \n",
    "    return os.path.join(save_dir, datafile_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def markdown_table(feature_dict):\n",
    "    return Markdown(tabulate(\n",
    "      [[v for v in row.values()] for row in feature_dict],\n",
    "      headers=[k for k in feature_dict[0].keys()],\n",
    "        tablefmt='github'\n",
    "    ))\n",
    "\n",
    "def markdown_flow_table(flow_table):\n",
    "    columns = set([C for S in flow_table for C in flow_table[S]])\n",
    "    print(columns)\n",
    "    return markdown_table()(\n",
    "        [\n",
    "            dict(file=fn, **{c:rd.get(c,' ') for c in columns}) \n",
    "            for fn, rd in flow_table.items()\n",
    "        ]    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def display_table(feature_dict):\n",
    "    display(markdown_table(feature_dict))\n",
    "    \n",
    "data_file_flow = {}\n",
    "\n",
    "def display_flow_table(flow_table):\n",
    "    columns = set([C for S in data_file_flow for C in data_file_flow[S]])\n",
    "    print(columns)\n",
    "    display(markdown_table(        [\n",
    "            dict(file=fn, **{c:rd.get(c,' ') for c in columns}) \n",
    "            for fn, rd in flow_table.items()\n",
    "        ]    \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running R code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts written in R can be run from a Python program using `subprocess` and `Rscript`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Rscript`\n",
    "\n",
    "A script can be run from the commandline with\n",
    "\n",
    "    Rscript ascript.R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `subproces`\n",
    "\n",
    "[Python's `subprocess`module](https://docs.python.org/3.11/library/subprocess.html#) has the tools to execute external programs like `Rscript`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1] \"hello world\"\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "subprocess.run(['Rscript',get_asset_path('hello.R', assets_dir)], capture_output=True).stdout.decode('UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: sapflow prediction scripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Data_preparation.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringer\n",
    "* zoo\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Data/Meta_data.csv`\n",
    "* `Data/Sapflux_Tilia_train.csv`\n",
    "* `Data/Weather_Tilia_train.csv`\n",
    "* `Data/Weather_Tilia_pred.csv`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Modelling_data.RData`\n",
    "* `Prediction_data.RData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Data_preparation.R'] = {\n",
    "      \"in\": [\n",
    "         \"Data/Meta_data.csv\",\n",
    "         \"Data/Sapflux_Tilia_train.csv\",\n",
    "         \"Data/Weather_Tilia_train.csv\",\n",
    "         \"Data/Weather_Tilia_pred.csv\"\n",
    "      ],\n",
    "      \"out\": [\n",
    "         \"Modelling_data.RData\",\n",
    "         \"Prediction_data.RData\"\n",
    "      ],\n",
    "    'libs':['lubridate', 'stringr', 'zoo']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part1.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "* mgcv\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Modelling_data.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Fitted_models.RData`\n",
    "* `Weights.RData`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part1.R'] = {\n",
    "      \"in\": [\n",
    "         \"Modelling_data.RData\"\n",
    "      ],\n",
    "      \"out\": [\n",
    "         \"Fitted_models.RData\",\n",
    "         \"Weights.RData\"\n",
    "      ],\n",
    "    'libs':['lubridate', 'stringr', 'mgcv']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part2.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "* mgcv\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Fitted_models.RData`\n",
    "* `Weights.RData`\n",
    "* `Modelling_data.RData`\n",
    "* `Prediction_data.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Predicted_sapflux.RData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part2.R'] = {\n",
    "    \"in\":[\n",
    "        'Fitted_models.RData',\n",
    "        'Weights.RData',\n",
    "        'Modelling_data.RData',\n",
    "        'Prediction_data.RData'\n",
    "    ],\n",
    "    \"out\":[\n",
    "        'Predicted_sapflux.RData'\n",
    "    ],\n",
    "    'libs':['lubridate', 'stringr', 'mgcv']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part3.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Predicted_sapflux.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Predicted_water_usage.RData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part3.R'] = {\n",
    "    'in':['Predicted_sapflux.RData'],\n",
    "    'out':['Predicted_water_usage.RData'],\n",
    "    'libs':['lubridate', 'stringr']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| data-file / script          | Data_preparation.R   | Prediction_part1.R   | Prediction_part2.R   | Prediction_part3.R   |\n",
       "|-----------------------------|----------------------|----------------------|----------------------|----------------------|\n",
       "| Meta_data.csv               | in                   | --                   | --                   | --                   |\n",
       "| Sapflux_Tilia_train.csv     | in                   | --                   | --                   | --                   |\n",
       "| Weather_Tilia_train.csv     | in                   | --                   | --                   | --                   |\n",
       "| Weather_Tilia_pred.csv      | in                   | --                   | --                   | --                   |\n",
       "| Modelling_data.RData        | out                  | in                   | in                   | --                   |\n",
       "| Prediction_data.RData       | out                  | --                   | in                   | --                   |\n",
       "| Fitted_models.RData         | --                   | out                  | in                   | --                   |\n",
       "| Weights.RData               | --                   | out                  | in                   | --                   |\n",
       "| Predicted_sapflux.RData     | --                   | --                   | out                  | in                   |\n",
       "| Predicted_water_usage.RData | --                   | --                   | --                   | out                  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "script_order = dict(zip(data_file_flow.keys(), range(len(data_file_flow.keys()))))\n",
    "\n",
    "# add the name to the objects\n",
    "data_file_flow = {\n",
    "    script_order[k]:{**v, 'name':k}\n",
    "    for k,v in data_file_flow.items()\n",
    "}\n",
    "\n",
    "\n",
    "data_files = reduce(\n",
    "    lambda Y,X:Y if (X in Y) else [*Y,X],\n",
    "    [\n",
    "        f\n",
    "        for S,P in data_file_flow.items() # patterns\n",
    "        for D,F in P.items()\n",
    "        for f in F\n",
    "        if D in ['in','out']\n",
    "        \n",
    "    ],\n",
    "    []\n",
    ")\n",
    "display(Markdown(tabulate(\n",
    "    [\n",
    "        [F.split('/')[-1]]+[\n",
    "            'in' if F in P['in'] else 'out' if F in P['out'] else '--' \n",
    "            for S,P in data_file_flow.items()\n",
    "        ] \n",
    "        for F in data_files\n",
    "    ],\n",
    "    headers=['data-file / script'] + [I['name'] for I in data_file_flow.values()],\n",
    "    tablefmt='github'\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/Meta_data.csv',\n",
       " 'Data/Sapflux_Tilia_train.csv',\n",
       " 'Data/Weather_Tilia_train.csv',\n",
       " 'Data/Weather_Tilia_pred.csv',\n",
       " 'Modelling_data.RData',\n",
       " 'Prediction_data.RData',\n",
       " 'Fitted_models.RData',\n",
       " 'Weights.RData',\n",
       " 'Predicted_sapflux.RData',\n",
       " 'Predicted_water_usage.RData']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'in': ['Data/Meta_data.csv',\n",
       "   'Data/Sapflux_Tilia_train.csv',\n",
       "   'Data/Weather_Tilia_train.csv',\n",
       "   'Data/Weather_Tilia_pred.csv'],\n",
       "  'out': ['Modelling_data.RData', 'Prediction_data.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'zoo'],\n",
       "  'name': 'Data_preparation.R'},\n",
       " 1: {'in': ['Modelling_data.RData'],\n",
       "  'out': ['Fitted_models.RData', 'Weights.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'mgcv'],\n",
       "  'name': 'Prediction_part1.R'},\n",
       " 2: {'in': ['Fitted_models.RData',\n",
       "   'Weights.RData',\n",
       "   'Modelling_data.RData',\n",
       "   'Prediction_data.RData'],\n",
       "  'out': ['Predicted_sapflux.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'mgcv'],\n",
       "  'name': 'Prediction_part2.R'},\n",
       " 3: {'in': ['Predicted_sapflux.RData'],\n",
       "  'out': ['Predicted_water_usage.RData'],\n",
       "  'libs': ['lubridate', 'stringr'],\n",
       "  'name': 'Prediction_part3.R'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import R libraries\n",
    "\n",
    "Importing libraries can be done with\n",
    "\n",
    "    Rscript -e 'install.packages(\"drat\", repos=\"https://cloud.r-project.org\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False, 'Stop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rscript (R) version 4.2.2 (2022-10-31)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subprocess.run(['Rscript','--version', ], capture_output=True).stdout.decode('UTF-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rscript (R) version 4.2.2 (2022-10-31)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rversion = subprocess.run(['Rscript','--version', ], capture_output=True)\n",
    "print(rversion.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lubridate', 'zoo', 'mgcv', 'stringr']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([L for V in data_file_flow.values() for L in V['libs']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘zoo’\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_script_result = subprocess.run(['Rscript','-e', \"library(zoo)\"], capture_output=True)\n",
    "print(run_script_result.stderr.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_rscript_env(libfolder:str):\n",
    "    if os.environ.get('R_LIBS_USER'):\n",
    "        return dict(**os.environ)\n",
    "    else:\n",
    "        return dict(**os.environ, R_LIBS_USER=libfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_rscript_env(get_rscript_libpath(save_dir)).get('R_LIBS_USER') == get_rscript_libpath(save_dir), 'rscript environment not set as expected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def check_rscript_libs(libs:list, libfolder:str):\n",
    "    return all([os.path.exists(os.path.join(libfolder, L)) for L in libs])\n",
    "\n",
    "def check_rscript_lib(lib:str, libfolder:str):\n",
    "    run_script_result = subprocess.run(['Rscript','-e', f\"library({lib})\"], env=get_rscript_env(libfolder), capture_output=True)\n",
    "    return run_script_result.returncode == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_rscript_lib('ueryoryeoyreow', get_rscript_libpath(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def install_R_package_wait(pkg:str|list, libfolder:str, repo='https://cloud.r-project.org'):\n",
    "    \"\"\"\n",
    "    Checks and if neccesary installs an R package\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pkg : str|list\n",
    "        name(s) of the package(s)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(pkg, str):\n",
    "        return install_R_package_wait([pkg], libfolder, repo)\n",
    "\n",
    "    os.makedirs(libfolder, exist_ok=True)\n",
    "    env = dict(os.environ)\n",
    "    env['R_LIBS_USER'] = libfolder\n",
    "    \n",
    "    for pkg_i in pkg: # ['generics', 'timechange', 'rlang', 'stringi'] + \n",
    "        print(f\"\\nInstalling package {pkg_i}, testing attach ...\")\n",
    "        if not check_rscript_lib(pkg_i, libfolder):\n",
    "            print(f\"Package {pkg_i} not attached. Installing {pkg_i}\")\n",
    "            run_script_install = subprocess.run([\n",
    "                    'Rscript','-e', \n",
    "                    f\"install.packages('{pkg_i}', repos='{repo}', lib='{libfolder}', dependencies=TRUE)\"\n",
    "                ], capture_output=True, env=env)\n",
    "            \n",
    "            if run_script_install.returncode != 0:\n",
    "                print(f\"installing {pkg_i}, returned code {run_script_install.returncode} ... \")\n",
    "                print('STDOUT--------------\\n', run_script_install.stdout.decode('UTF-8'))\n",
    "                print('STDERR--------------\\n', run_script_install.stderr.decode('UTF-8'))\n",
    "\n",
    "            elif not check_rscript_lib(pkg_i, libfolder): # not in cache\n",
    "                print(f\"Attach after installing for {pkg_i} failed ... install logs below\")\n",
    "                print('STDOUT--------------\\n', run_script_install.stdout.decode('UTF-8'))\n",
    "                print('STDERR--------------\\n', run_script_install.stderr.decode('UTF-8'))\n",
    "            else:\n",
    "                print(f\"Attach after installation was successful. Library {pkg_i} appears to have been installed\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Attach successful. Library {pkg_i} appears to have been installed\")\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing package zoo, testing attach ...\n",
      "Attach successful. Library zoo appears to have been installed\n"
     ]
    }
   ],
   "source": [
    "install_R_package_wait(\n",
    "    ['zoo'],\n",
    "    libfolder=get_rscript_libpath(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing package lubridate, testing attach ...\n",
      "Attach successful. Library lubridate appears to have been installed\n",
      "\n",
      "Installing package mgcv, testing attach ...\n",
      "Attach successful. Library mgcv appears to have been installed\n",
      "\n",
      "Installing package stringr, testing attach ...\n",
      "Attach successful. Library stringr appears to have been installed\n",
      "\n",
      "Installing package zoo, testing attach ...\n",
      "Attach successful. Library zoo appears to have been installed\n"
     ]
    }
   ],
   "source": [
    "install_R_package_wait(\n",
    "    sorted(list(set([L for V in data_file_flow.values() for L in V['libs']]))),\n",
    "    libfolder=get_rscript_libpath(save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checksum calculation\n",
    "\n",
    "Each script has it's own set of input files and should be run to\n",
    "update it's output when either it's inputs have changed or it's \n",
    "expected output does not exist.\n",
    "\n",
    "We can check for filechanges using a hashing algorithm, for \n",
    "instance MD5 or SHA-256. These are available either in Python\n",
    "or from the commandline.\n",
    "\n",
    "Lets look at the commandline version of MD5, on linux this is\n",
    "`md5sum`, with the input file for the preparation stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   \"Data/Meta_data.csv\",\n",
      "   \"Data/Sapflux_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_pred.csv\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data_file_flow[list(data_file_flow.keys())[0]]['in'], indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "md5sum will output hashes to stdout, which `subprocess.run` captures for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   \"Data/Meta_data.csv\",\n",
      "   \"Data/Sapflux_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_pred.csv\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "flow_object_index = 0\n",
    "input_files = data_file_flow[flow_object_index]['in']\n",
    "\n",
    "print(json.dumps(input_files, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4bed61a77505bfd52032591d5c3a6050 *Data/Meta_data.csv\n",
      "6d705d98caa6618a4a990c3742c16564 *Data/Sapflux_Tilia_train.csv\n",
      "1232592f9488ce4fbb4ae11ba5be0349 *Data/Weather_Tilia_train.csv\n",
      "366dac1bf64003d1d08fca6121c036bd *Data/Weather_Tilia_pred.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "md5_encode_result = subprocess.run(\n",
    "    ['md5sum','-b']+\n",
    "    input_files, \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_encode_result.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to check the files we run it with the `-c` option and a file with the previously calculated checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = data_file_flow[flow_object_index]['name']\n",
    "\n",
    "checksum_file = get_save_path(f\"input-checksum-{script_name.split('.')[0]}\", save_dir)\n",
    "with open(checksum_file, 'wt') as cf:\n",
    "    cf.write(md5_encode_result.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Meta_data.csv: OK\n",
      "Data/Sapflux_Tilia_train.csv: OK\n",
      "Data/Weather_Tilia_train.csv: OK\n",
      "Data/Weather_Tilia_pred.csv: OK\n",
      "\n",
      "Run returned code 0\n"
     ]
    }
   ],
   "source": [
    "md5_check_result = subprocess.run(\n",
    "    ['md5sum', '-c', checksum_file], \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_check_result.stdout.decode('UTF-8'))\n",
    "print(f\"Run returned code {md5_check_result.returncode}\")\n",
    "if md5_check_result.returncode:\n",
    "    print(md5_check_result.stderr.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had there been a change to a file it would have looked like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run returned code 1\n"
     ]
    }
   ],
   "source": [
    "md5_check_result = subprocess.run(\n",
    "    ['md5sum', '-c', checksum_file+'-modified'], \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_check_result.stdout.decode('UTF-8'))\n",
    "print(f\"Run returned code {md5_check_result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't really need specifics, only the return code will\n",
    "do for our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking files\n",
    "\n",
    "\n",
    "#### Generating names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "read_chunk_size = 1024 * 32\n",
    "def calc_hash_from_flowobject(flow_object:dict)->str:\n",
    "    '''Calculate a unique hash for a given flow object'''\n",
    "    return hashlib.md5(repr(flow_object).encode('UTF-8')).hexdigest()\n",
    "\n",
    "def calc_hash_from_files(files:list, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files'''\n",
    "    hashobj = hashlib.md5()\n",
    "\n",
    "    # iterate over files \n",
    "    for data_file in files:\n",
    "        full_name = os.path.join(save_dir, data_file)\n",
    "        if not os.path.isfile(full_name):\n",
    "            continue\n",
    "        \n",
    "        with open(full_name, 'rb') as f:\n",
    "            # loop till the end of the file\n",
    "            while True:\n",
    "                # read only 1024 bytes at a time\n",
    "                chunk = f.read(read_chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                hashobj.update(chunk)\n",
    "        \n",
    "    return hashobj.hexdigest()\n",
    "\n",
    "def calc_hash_from_input_files(flow_object:dict, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files for a given flow object'''\n",
    "    return calc_hash_from_files(flow_object['in'], save_dir)\n",
    "\n",
    "def calc_hash_from_data_files(flow_object:dict, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files for a given flow object'''\n",
    "    return calc_hash_from_files(flow_object['in'] + flow_object['out'], save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'da4b2413f6a22c19a8a7823e6564e746'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_flowobject(data_file_flow[flow_object_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32095cd16a83a2c63f1ab51a58ed96c9'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_input_files(data_file_flow[flow_object_index], save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'65e6d36ac0c8aadb1fb4ee48d4ff88f3'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_data_files(data_file_flow[flow_object_index], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def check_script_inputs(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\" \n",
    "    Check if the input files for a script are up-to-date, returns True if up-to-date.\n",
    "    \"\"\"\n",
    "\n",
    "    checksum_file = get_save_path(f\"input-checksum-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    md5_check_result = subprocess.run(\n",
    "        ['md5sum', '-c', checksum_file], \n",
    "        cwd=save_dir,\n",
    "        capture_output=True)\n",
    "    \n",
    "    return int(md5_check_result.returncode) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_script_inputs(data_file_flow[0], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputs\n",
    "The output is easily checked for existence with `isfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def check_script_output(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\" \n",
    "    Check if the output files for a script exist, returns True if they all exist.\n",
    "    \"\"\"\n",
    "\n",
    "    return all([\n",
    "        os.path.isfile(get_save_path(F, save_dir)) \n",
    "        for F in flow_object['out']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_script_output(data_file_flow[0], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the checksum file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def generate_checksum_file(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\"Generates the checksum file for a given flow object\"\"\"\n",
    "\n",
    "    input_files = flow_object['in']\n",
    "    md5_encode_result = subprocess.run(\n",
    "        ['md5sum','-b']+\n",
    "        input_files, \n",
    "        cwd=save_dir,\n",
    "        capture_output=True)\n",
    "    \n",
    "    checksum_file = get_save_path(f\"input-checksum-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    with open(checksum_file, 'wt') as cf:\n",
    "        cf.write(md5_encode_result.stdout.decode('UTF-8'))\n",
    "\n",
    "    return md5_encode_result.returncode == 0 and check_script_inputs(flow_object, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_checksum_file(data_file_flow[0], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running once\n",
    "\n",
    "We don't need and don't _want't_ to run the script if more then once. This \n",
    "is not a problem when a script has finished and updated the checksum file, \n",
    "but we also want to prevent near simultaneous runs in a multiprocessing \n",
    "environment.\n",
    "\n",
    "We'll use file locking from fcntl directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use fcntl file locking to prevent multiple processes from running the same code at the same time.\n",
    "# see https://docs.python.org/3/library/fcntl.html#fcntl.flock\n",
    "\n",
    "# Create a filename based on input-file contents\n",
    "lock_file = get_save_path(f\"lock-{calc_hash_from_input_files(data_file_flow[0], save_dir)}\", save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `fcntl.flock` with flags `fcntl.LOCK_EX | fcntl.LOCK_NB` to lock the file for exclusive access, while\n",
    "an exception is thrown if it's already locked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 11] Resource temporarily unavailable\n"
     ]
    }
   ],
   "source": [
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "    with open(lock_file, 'wt') as cf2:\n",
    "        try:\n",
    "            fcntl.flock(cf2, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "        except BlockingIOError as locked_error:\n",
    "            print(locked_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locks are removed when the file is closed, how convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "### Synchroneous\n",
    "\n",
    "We need to run a script when either any of it's inputs have changed or any \n",
    "of it's outputs do not exist. Return True if a follow-up script should be \n",
    "executed, False if nothing changed or executing the script failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def run_rscript_wait(flow_object, assets_dir:str, save_dir:str):\n",
    "    \"\"\" Run a script in R \n",
    "        args:\n",
    "            flow_object: dict of flow object\n",
    "        returns:\n",
    "            bool: True if a follow-up script might need to be run, False if not\n",
    "\n",
    "    \"\"\"\n",
    "    syslog.debug(f\"Running script {flow_object['name']}\")\n",
    "    # Check if output exists and inputs have not changed and return False if \n",
    "    # output exists and inputs have not changed\n",
    "    if check_script_output(flow_object, save_dir) and check_script_inputs(flow_object, save_dir):\n",
    "        return True\n",
    "    \n",
    "    # Create the lock file\n",
    "    lock_file = get_save_path(f\"lock-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    with open(lock_file, 'wt') as cf:\n",
    "        try:\n",
    "            syslog.debug(f\"Locking {lock_file}\")\n",
    "            # Get exclusive lock on the file, is released on file close\n",
    "            fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "\n",
    "            # run the script\n",
    "            run_script_result = subprocess.run(\n",
    "                ['Rscript', '--vanilla', get_asset_path(flow_object['name'], assets_dir)],\n",
    "                cwd=save_dir,\n",
    "                capture_output=True\n",
    "            )\n",
    "            \n",
    "            # check the return code\n",
    "            if run_script_result.returncode:\n",
    "                cf.write(f\"Run returned code {run_script_result.returncode}\\n\")\n",
    "                cf.write(f\"STDOUT------------\\n{run_script_result.stdout.decode('UTF-8')}\\n\")\n",
    "                cf.write(f\"STDERR------------\\n{run_script_result.stderr.decode('UTF-8')}\\n\")\n",
    "                return False\n",
    "\n",
    "        except BlockingIOError as locked_error:\n",
    "            syslog.debug(locked_error)\n",
    "            return False\n",
    "\n",
    "    \n",
    "    # check the output and generate the checksum file\n",
    "    return check_script_output(flow_object, save_dir) and generate_checksum_file(flow_object, save_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Running script Data_preparation.R\n",
      "2024-08-15T16:51:49+0200 DEBUG\t10995\troot\t2858783497.py\t11\tRunning script Data_preparation.R\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rscript_wait(data_file_flow[0], assets_dir, save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Running script Prediction_part1.R\n",
      "2024-08-15T16:51:49+0200 DEBUG\t10995\troot\t2858783497.py\t11\tRunning script Prediction_part1.R\n",
      "DEBUG:root:Locking /home/fenke/repos/corebridge/nbs/saves/rscript/lock-6ea0e8b3895468772af891ee7b90a11e\n",
      "2024-08-15T16:51:49+0200 DEBUG\t10995\troot\t2858783497.py\t21\tLocking /home/fenke/repos/corebridge/nbs/saves/rscript/lock-6ea0e8b3895468772af891ee7b90a11e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rscript_wait(data_file_flow[1], assets_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Running script Prediction_part2.R\n",
      "2024-08-15T16:51:49+0200 DEBUG\t10995\troot\t2858783497.py\t11\tRunning script Prediction_part2.R\n",
      "DEBUG:root:Locking /home/fenke/repos/corebridge/nbs/saves/rscript/lock-70dfaa2a3c99ed6ac2c53df7e144b4d8\n",
      "2024-08-15T16:51:49+0200 DEBUG\t10995\troot\t2858783497.py\t21\tLocking /home/fenke/repos/corebridge/nbs/saves/rscript/lock-70dfaa2a3c99ed6ac2c53df7e144b4d8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rscript_wait(data_file_flow[2], assets_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_results(flow_object :dict, save_dir:str):\n",
    "    \"\"\"Clear the results of a given flow object\"\"\"\n",
    "    for fname in flow_object['out']:\n",
    "        try:\n",
    "            os.remove(get_save_path(fname, save_dir))\n",
    "        except FileNotFoundError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clear_results(O, save_dir) for O in data_file_flow.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous\n",
    "\n",
    "In the API we can not wait for a script to finish and we'll use Popen instead. This\n",
    "means we'll have to keep track of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "RScriptProcess = namedtuple('RScriptProcess', ['lock_file', 'popen'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous RScript processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def run_rscript_nowait(flow_object, assets_dir:str, save_dir:str, pkg_repo:str='https://cloud.r-project.org') -> RScriptProcess:\n",
    "    \"\"\" Run a script in R \n",
    "        args:\n",
    "            flow_object: dict of flow object\n",
    "            assets_dir: path to the assets directory\n",
    "            save_dir: path to the save directory\n",
    "        returns:\n",
    "            RScriptProcess: Popen container object for the script\n",
    "    \"\"\"\n",
    "    \n",
    "    syslog.debug(f\"Starting script {flow_object['name']}\")\n",
    "\n",
    "    # check lockfile ---------------------------------------------------------------\n",
    "    # do lock maintenance\n",
    "    lock_name = 'run_flow_object-'+calc_hash_from_flowobject(flow_object)\n",
    "    if run_rscript_nowait.lock_objects.get(lock_name): \n",
    "        lock_object = run_rscript_nowait.lock_objects[lock_name]\n",
    "        if not lock_object.lock_file.closed:\n",
    "            syslog.debug(f\"Lockfile is open for {flow_object['name']} ({lock_name})\")\n",
    "            \n",
    "            if lock_object.popen is None:\n",
    "                syslog.debug(f\"No process running for {flow_object['name']} ({lock_name})\")\n",
    "            elif lock_object.popen.poll() is None:\n",
    "                syslog.debug(f\"Script is still running for {flow_object['name']} ({lock_name})\")\n",
    "                return lock_object\n",
    "            else:\n",
    "                syslog.debug(f\"Script has finished for {flow_object['name']} ({lock_name}), returned {lock_object.popen.returncode}\")\n",
    "                # since poll return not-None the script has finished so close the lockfile\n",
    "                lock_object.lock_file.close()\n",
    "                #run_rscript_nowait.lock_objects.pop(lock_name)\n",
    "\n",
    "    # Check if output exists and inputs have not changed and return False if \n",
    "    # output exists and inputs have not changed\n",
    "    if check_script_output(flow_object, save_dir) and check_script_inputs(flow_object, save_dir):\n",
    "        syslog.debug(f\"Output and inputs are up-to-date for {flow_object['name']}\")\n",
    "        return run_rscript_nowait.lock_objects.get(lock_name)\n",
    "\n",
    "    # Create the lock file -----------------------------------------------------------\n",
    "    syslog.debug(f\"Creating lockfile for {flow_object['name']} ({lock_name})\")\n",
    "    cf = open(get_save_path(f\"lock-{lock_name}\", save_dir), 'wt')\n",
    "    \n",
    "    try:\n",
    "        # Set lock on lockfile\n",
    "        fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "\n",
    "        # check libs\n",
    "        libfolder=get_rscript_libpath(save_dir)\n",
    "        os.makedirs(libfolder, exist_ok=True)\n",
    "        \n",
    "        env = dict(os.environ)\n",
    "        env['R_LIBS_USER'] = libfolder\n",
    "\n",
    "        if not check_rscript_libs(flow_object['libs'], libfolder):\n",
    "            syslog.debug(f\"Installing libs for {flow_object['name']} ({lock_name}): {flow_object['libs']}\")\n",
    "            for pkg_i in flow_object['libs']:\n",
    "                if not check_rscript_lib(pkg_i, libfolder):\n",
    "                    syslog.debug(f\"Installing {pkg_i} for {flow_object['name']} ({lock_name})\")\n",
    "                    run_script_install = subprocess.Popen()([\n",
    "                            'Rscript','-e', \n",
    "                            f\"install.packages('{pkg_i}', repos='{pkg_repo}', lib='{libfolder}', dependencies=TRUE)\"\n",
    "                        ], \n",
    "                        cwd=save_dir,\n",
    "                        stdout=subprocess.PIPE,\n",
    "                        stderr=subprocess.PIPE,\n",
    "                        encoding='UTF-8',\n",
    "                        env=env)\n",
    "                    run_rscript_nowait.lock_objects[lock_name] =  RScriptProcess(cf, run_script_install)\n",
    "                    return run_rscript_nowait.lock_objects.get(lock_name)\n",
    "                    \n",
    "        \n",
    "        syslog.debug(f\"Libs are up-to-date, running script for {flow_object['name']} ({lock_name})\")\n",
    "        # run the script\n",
    "        popen = subprocess.Popen(\n",
    "            ['Rscript', '--vanilla', get_asset_path(flow_object['name'], assets_dir)],\n",
    "            cwd=save_dir,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            encoding='UTF-8',\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "        run_rscript_nowait.lock_objects[lock_name] =  RScriptProcess(cf, popen)\n",
    "            \n",
    "    except BlockingIOError as locked_error:\n",
    "        cf.close()\n",
    "        syslog.error(locked_error)\n",
    "\n",
    "    syslog.debug(f\"Done with {flow_object['name']}\")\n",
    "\n",
    "    return run_rscript_nowait.lock_objects.get(lock_name)\n",
    "\n",
    "run_rscript_nowait.lock_objects = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Starting script Prediction_part2.R\n",
      "2024-08-15T16:51:49+0200 DEBUG\t10995\troot\t4102004212.py\t12\tStarting script Prediction_part2.R\n",
      "DEBUG:root:Creating lockfile for Prediction_part2.R (run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8)\n",
      "2024-08-15T16:51:49+0200 DEBUG\t10995\troot\t4102004212.py\t40\tCreating lockfile for Prediction_part2.R (run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8)\n",
      "DEBUG:root:Installing libs for Prediction_part2.R (run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8): ['lubridate', 'stringr', 'mgcv']\n",
      "2024-08-15T16:51:49+0200 DEBUG\t10995\troot\t4102004212.py\t55\tInstalling libs for Prediction_part2.R (run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8): ['lubridate', 'stringr', 'mgcv']\n",
      "DEBUG:root:Libs are up-to-date, running script for Prediction_part2.R (run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8)\n",
      "2024-08-15T16:51:51+0200 DEBUG\t10995\troot\t4102004212.py\t72\tLibs are up-to-date, running script for Prediction_part2.R (run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8)\n",
      "DEBUG:root:Done with Prediction_part2.R\n",
      "2024-08-15T16:51:51+0200 DEBUG\t10995\troot\t4102004212.py\t89\tDone with Prediction_part2.R\n"
     ]
    }
   ],
   "source": [
    "run_result = run_rscript_nowait(data_file_flow[2], assets_dir, save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RScriptProcess(lock_file=<_io.TextIOWrapper name='/home/fenke/repos/corebridge/nbs/saves/rscript/lock-run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8' mode='wt' encoding='UTF-8'>, popen=<Popen: returncode: None args: ['Rscript', '--vanilla', '/home/fenke/repos/c...>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_result.popen:\n",
    "    run_result.popen.poll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_result.popen:\n",
    "    run_result.popen.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_result.popen:\n",
    "    run_result.popen.stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing lockfile /home/fenke/repos/corebridge/nbs/saves/rscript/lock-run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8\n"
     ]
    }
   ],
   "source": [
    "if run_result.popen and run_result.popen.poll() is not None:\n",
    "    print(f\"Closing lockfile {run_result.lock_file.name}\")\n",
    "    run_result.lock_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Starting script Data_preparation.R\n",
      "2024-08-15T16:51:52+0200 DEBUG\t10995\troot\t4102004212.py\t12\tStarting script Data_preparation.R\n",
      "DEBUG:root:Creating lockfile for Data_preparation.R (run_flow_object-da4b2413f6a22c19a8a7823e6564e746)\n",
      "2024-08-15T16:51:52+0200 DEBUG\t10995\troot\t4102004212.py\t40\tCreating lockfile for Data_preparation.R (run_flow_object-da4b2413f6a22c19a8a7823e6564e746)\n",
      "DEBUG:root:Libs are up-to-date, running script for Data_preparation.R (run_flow_object-da4b2413f6a22c19a8a7823e6564e746)\n",
      "2024-08-15T16:51:52+0200 DEBUG\t10995\troot\t4102004212.py\t72\tLibs are up-to-date, running script for Data_preparation.R (run_flow_object-da4b2413f6a22c19a8a7823e6564e746)\n",
      "DEBUG:root:Done with Data_preparation.R\n",
      "2024-08-15T16:51:52+0200 DEBUG\t10995\troot\t4102004212.py\t89\tDone with Data_preparation.R\n"
     ]
    }
   ],
   "source": [
    "test = run_rscript_nowait(data_file_flow[0], assets_dir, save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RScriptProcess(lock_file=<_io.TextIOWrapper name='/home/fenke/repos/corebridge/nbs/saves/rscript/lock-run_flow_object-da4b2413f6a22c19a8a7823e6564e746' mode='wt' encoding='UTF-8'>, popen=<Popen: returncode: None args: ['Rscript', '--vanilla', '/home/fenke/repos/c...>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test and test.popen:\n",
    "    test.popen.poll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AICore module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AICoreRModule(AICoreModule):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corebridge.venv",
   "language": "python",
   "name": "corebridge.venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
