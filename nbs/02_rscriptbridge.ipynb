{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RScript-Bridge\n",
    "\n",
    "> Bridge between Stactics AICore framework and RScript prediction scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some things to set up first\n",
    "\n",
    "Notebooks use nbdev thingses and `addroot` makes importing from\n",
    "the repo-directory more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import logging\n",
    "#import addroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corebridge.aicorebridge 0.2.17 from /home/fenke/repos/corebridge/corebridge/aicorebridge.py\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import json, os, fcntl, logging\n",
    "import subprocess\n",
    "import hashlib\n",
    "from collections import namedtuple\n",
    "\n",
    "from corebridge.aicorebridge import AICoreModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AICore uses an `assets` dir from which we can read files, like scripts\n",
    "and a `save` dir were modules can write and read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "syslog = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assets_dir = os.path.join(os.path.abspath(os.getcwd()), 'assets', 'rscript')\n",
    "save_dir = os.path.join(os.path.abspath(os.getcwd()), 'saves', 'rscript')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def get_asset_path(script_name, assets_dir:str): \n",
    "    return os.path.join(assets_dir, script_name)\n",
    "def get_save_path(datafile_name, save_dir): \n",
    "    return os.path.join(save_dir, datafile_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def markdown_table(feature_dict):\n",
    "    return Markdown(tabulate(\n",
    "      [[v for v in row.values()] for row in feature_dict],\n",
    "      headers=[k for k in feature_dict[0].keys()],\n",
    "        tablefmt='github'\n",
    "    ))\n",
    "\n",
    "def markdown_flow_table(flow_table):\n",
    "    columns = set([C for S in flow_table for C in flow_table[S]])\n",
    "    print(columns)\n",
    "    return markdown_table()(\n",
    "        [\n",
    "            dict(file=fn, **{c:rd.get(c,' ') for c in columns}) \n",
    "            for fn, rd in flow_table.items()\n",
    "        ]    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def display_table(feature_dict):\n",
    "    display(markdown_table(feature_dict))\n",
    "    \n",
    "data_file_flow = {}\n",
    "\n",
    "def display_flow_table(flow_table):\n",
    "    columns = set([C for S in data_file_flow for C in data_file_flow[S]])\n",
    "    print(columns)\n",
    "    display(markdown_table(        [\n",
    "            dict(file=fn, **{c:rd.get(c,' ') for c in columns}) \n",
    "            for fn, rd in flow_table.items()\n",
    "        ]    \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running R code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts written in R can be run from a Python program using `subprocess` and `Rscript`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Rscript`\n",
    "\n",
    "A script can be run from the commandline with\n",
    "\n",
    "    Rscript ascript.R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `subproces`\n",
    "\n",
    "[Python's `subprocess`module](https://docs.python.org/3.11/library/subprocess.html#) has the tools to execute external programs like `Rscript`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1] \"hello world\"\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "subprocess.run(['Rscript',get_asset_path('hello.R', assets_dir)], capture_output=True).stdout.decode('UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: sapflow prediction scripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Data_preparation.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringer\n",
    "* zoo\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Data/Meta_data.csv`\n",
    "* `Data/Sapflux_Tilia_train.csv`\n",
    "* `Data/Weather_Tilia_train.csv`\n",
    "* `Data/Weather_Tilia_pred.csv`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Modelling_data.RData`\n",
    "* `Prediction_data.RData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Data_preparation.R'] = {\n",
    "      \"in\": [\n",
    "         \"Data/Meta_data.csv\",\n",
    "         \"Data/Sapflux_Tilia_train.csv\",\n",
    "         \"Data/Weather_Tilia_train.csv\",\n",
    "         \"Data/Weather_Tilia_pred.csv\"\n",
    "      ],\n",
    "      \"out\": [\n",
    "         \"Modelling_data.RData\",\n",
    "         \"Prediction_data.RData\"\n",
    "      ],\n",
    "    'libs':['lubridate', 'stringr', 'zoo']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part1.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "* mgcv\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Modelling_data.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Fitted_models.RData`\n",
    "* `Weights.RData`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part1.R'] = {\n",
    "      \"in\": [\n",
    "         \"Modelling_data.RData\"\n",
    "      ],\n",
    "      \"out\": [\n",
    "         \"Fitted_models.RData\",\n",
    "         \"Weights.RData\"\n",
    "      ],\n",
    "    'libs':['lubridate', 'stringr', 'mgcv']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part2.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "* mgcv\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Fitted_models.RData`\n",
    "* `Weights.RData`\n",
    "* `Modelling_data.RData`\n",
    "* `Prediction_data.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Predicted_sapflux.RData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part2.R'] = {\n",
    "    \"in\":[\n",
    "        'Fitted_models.RData',\n",
    "        'Weights.RData',\n",
    "        'Modelling_data.RData',\n",
    "        'Prediction_data.RData'\n",
    "    ],\n",
    "    \"out\":[\n",
    "        'Predicted_sapflux.RData'\n",
    "    ],\n",
    "    'libs':['lubridate', 'stringr', 'mgcv']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part3.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Predicted_sapflux.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Predicted_water_usage.RData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part3.R'] = {\n",
    "    'in':['Predicted_sapflux.RData'],\n",
    "    'out':['Predicted_water_usage.RData'],\n",
    "    'libs':['lubridate', 'stringr']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| data-file / script          | Data_preparation.R   | Prediction_part1.R   | Prediction_part2.R   | Prediction_part3.R   |\n",
       "|-----------------------------|----------------------|----------------------|----------------------|----------------------|\n",
       "| Meta_data.csv               | in                   | --                   | --                   | --                   |\n",
       "| Sapflux_Tilia_train.csv     | in                   | --                   | --                   | --                   |\n",
       "| Weather_Tilia_train.csv     | in                   | --                   | --                   | --                   |\n",
       "| Weather_Tilia_pred.csv      | in                   | --                   | --                   | --                   |\n",
       "| Modelling_data.RData        | out                  | in                   | in                   | --                   |\n",
       "| Prediction_data.RData       | out                  | --                   | in                   | --                   |\n",
       "| Fitted_models.RData         | --                   | out                  | in                   | --                   |\n",
       "| Weights.RData               | --                   | out                  | in                   | --                   |\n",
       "| Predicted_sapflux.RData     | --                   | --                   | out                  | in                   |\n",
       "| Predicted_water_usage.RData | --                   | --                   | --                   | out                  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "script_order = dict(zip(data_file_flow.keys(), range(len(data_file_flow.keys()))))\n",
    "\n",
    "# add the name to the objects\n",
    "data_file_flow = {\n",
    "    script_order[k]:{**v, 'name':k}\n",
    "    for k,v in data_file_flow.items()\n",
    "}\n",
    "\n",
    "\n",
    "data_files = reduce(\n",
    "    lambda Y,X:Y if (X in Y) else [*Y,X],\n",
    "    [\n",
    "        f\n",
    "        for S,P in data_file_flow.items() # patterns\n",
    "        for D,F in P.items()\n",
    "        for f in F\n",
    "        if D in ['in','out']\n",
    "        \n",
    "    ],\n",
    "    []\n",
    ")\n",
    "display(Markdown(tabulate(\n",
    "    [\n",
    "        [F.split('/')[-1]]+[\n",
    "            'in' if F in P['in'] else 'out' if F in P['out'] else '--' \n",
    "            for S,P in data_file_flow.items()\n",
    "        ] \n",
    "        for F in data_files\n",
    "    ],\n",
    "    headers=['data-file / script'] + [I['name'] for I in data_file_flow.values()],\n",
    "    tablefmt='github'\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/Meta_data.csv',\n",
       " 'Data/Sapflux_Tilia_train.csv',\n",
       " 'Data/Weather_Tilia_train.csv',\n",
       " 'Data/Weather_Tilia_pred.csv',\n",
       " 'Modelling_data.RData',\n",
       " 'Prediction_data.RData',\n",
       " 'Fitted_models.RData',\n",
       " 'Weights.RData',\n",
       " 'Predicted_sapflux.RData',\n",
       " 'Predicted_water_usage.RData']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'in': ['Data/Meta_data.csv',\n",
       "   'Data/Sapflux_Tilia_train.csv',\n",
       "   'Data/Weather_Tilia_train.csv',\n",
       "   'Data/Weather_Tilia_pred.csv'],\n",
       "  'out': ['Modelling_data.RData', 'Prediction_data.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'zoo'],\n",
       "  'name': 'Data_preparation.R'},\n",
       " 1: {'in': ['Modelling_data.RData'],\n",
       "  'out': ['Fitted_models.RData', 'Weights.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'mgcv'],\n",
       "  'name': 'Prediction_part1.R'},\n",
       " 2: {'in': ['Fitted_models.RData',\n",
       "   'Weights.RData',\n",
       "   'Modelling_data.RData',\n",
       "   'Prediction_data.RData'],\n",
       "  'out': ['Predicted_sapflux.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'mgcv'],\n",
       "  'name': 'Prediction_part2.R'},\n",
       " 3: {'in': ['Predicted_sapflux.RData'],\n",
       "  'out': ['Predicted_water_usage.RData'],\n",
       "  'libs': ['lubridate', 'stringr'],\n",
       "  'name': 'Prediction_part3.R'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import R libraries\n",
    "\n",
    "Importing libraries can be done with\n",
    "\n",
    "    Rscript -e 'install.packages(\"drat\", repos=\"https://cloud.r-project.org\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rscript (R) version 4.2.2 (2022-10-31)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subprocess.run(['Rscript','--version', ], capture_output=True).stdout.decode('UTF-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rscript (R) version 4.2.2 (2022-10-31)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rversion = subprocess.run(['Rscript','--version', ], capture_output=True)\n",
    "print(rversion.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zoo', 'lubridate', 'mgcv', 'stringr']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([L for V in data_file_flow.values() for L in V['libs']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: nlme\n",
      "This is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_script_result = subprocess.run(['Rscript','-e', \"library(mgcv)\"], capture_output=True)\n",
    "print(run_script_result.stderr.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def install_R_package(pkg:str|list):\n",
    "    \"\"\"\n",
    "    Checks and if neccesary installs an R package\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pkg : str|list\n",
    "        name(s) of the package(s)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(pkg, str):\n",
    "        pkg = [pkg]\n",
    "\n",
    "    for pkg_i in pkg:\n",
    "        print(f\"Installing package {pkg_i} ...\")\n",
    "        run_script_result = subprocess.run(['Rscript','-e', f\"library({pkg_i})\"], capture_output=True)\n",
    "        if run_script_result.returncode != 0:\n",
    "            print(f\"Installing {pkg_i}\")\n",
    "            run_script_result = subprocess.run(['Rscript','-e', f\"install.packages({pkg_i}, repos='https://cloud.r-project.org')\"], capture_output=True)\n",
    "        else:\n",
    "            print(f\"Library {pkg_i} already installed\")\n",
    "            \n",
    "        print(run_script_result.stderr.decode('UTF-8'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Library zoo already installed\n",
      "DEBUG:__main__:\n",
      "Attaching package: ‘zoo’\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "\n",
      "INFO:__main__:Library lubridate already installed\n",
      "DEBUG:__main__:\n",
      "Attaching package: ‘lubridate’\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    date, intersect, setdiff, union\n",
      "\n",
      "\n",
      "INFO:__main__:Library mgcv already installed\n",
      "DEBUG:__main__:Loading required package: nlme\n",
      "This is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n",
      "\n",
      "INFO:__main__:Library stringr already installed\n",
      "DEBUG:__main__:\n"
     ]
    }
   ],
   "source": [
    "install_R_package(list(set([L for V in data_file_flow.values() for L in V['libs']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checksum calculation\n",
    "\n",
    "Each script has it's own set of input files and should be run to\n",
    "update it's output when either it's inputs have changed or it's \n",
    "expected output does not exist.\n",
    "\n",
    "We can check for filechanges using a hashing algorithm, for \n",
    "instance MD5 or SHA-256. These are available either in Python\n",
    "or from the commandline.\n",
    "\n",
    "Lets look at the commandline version of MD5, on linux this is\n",
    "`md5sum`, with the input file for the preparation stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   \"Data/Meta_data.csv\",\n",
      "   \"Data/Sapflux_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_pred.csv\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data_file_flow[list(data_file_flow.keys())[0]]['in'], indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "md5sum will output hashes to stdout, which `subprocess.run` captures for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   \"Data/Meta_data.csv\",\n",
      "   \"Data/Sapflux_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_pred.csv\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "flow_object_index = 0\n",
    "input_files = data_file_flow[flow_object_index]['in']\n",
    "\n",
    "print(json.dumps(input_files, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4bed61a77505bfd52032591d5c3a6050 *Data/Meta_data.csv\n",
      "6d705d98caa6618a4a990c3742c16564 *Data/Sapflux_Tilia_train.csv\n",
      "1232592f9488ce4fbb4ae11ba5be0349 *Data/Weather_Tilia_train.csv\n",
      "366dac1bf64003d1d08fca6121c036bd *Data/Weather_Tilia_pred.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "md5_encode_result = subprocess.run(\n",
    "    ['md5sum','-b']+\n",
    "    input_files, \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_encode_result.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to check the files we run it with the `-c` option and a file with the previously calculated checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = data_file_flow[flow_object_index]['name']\n",
    "\n",
    "checksum_file = get_save_path(f\"input-checksum-{script_name.split('.')[0]}\", save_dir)\n",
    "with open(checksum_file, 'wt') as cf:\n",
    "    cf.write(md5_encode_result.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Meta_data.csv: OK\n",
      "Data/Sapflux_Tilia_train.csv: OK\n",
      "Data/Weather_Tilia_train.csv: OK\n",
      "Data/Weather_Tilia_pred.csv: OK\n",
      "\n",
      "Run returned code 0\n"
     ]
    }
   ],
   "source": [
    "md5_check_result = subprocess.run(\n",
    "    ['md5sum', '-c', checksum_file], \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_check_result.stdout.decode('UTF-8'))\n",
    "print(f\"Run returned code {md5_check_result.returncode}\")\n",
    "if md5_check_result.returncode:\n",
    "    print(md5_check_result.stderr.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had there been a change to a file it would have looked like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run returned code 1\n"
     ]
    }
   ],
   "source": [
    "md5_check_result = subprocess.run(\n",
    "    ['md5sum', '-c', checksum_file+'-modified'], \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_check_result.stdout.decode('UTF-8'))\n",
    "print(f\"Run returned code {md5_check_result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't really need specifics, only the return code will\n",
    "do for our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking files\n",
    "\n",
    "\n",
    "#### Generating names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "read_chunk_size = 1024 * 32\n",
    "def calc_hash_from_flowobject(flow_object:dict)->str:\n",
    "    '''Calculate a unique hash for a given flow object'''\n",
    "    return hashlib.md5(repr(flow_object).encode('UTF-8')).hexdigest()\n",
    "\n",
    "def calc_hash_from_files(files:list, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files'''\n",
    "    hashobj = hashlib.md5()\n",
    "\n",
    "    # iterate over files \n",
    "    for data_file in files:\n",
    "        full_name = os.path.join(save_dir, data_file)\n",
    "        if not os.path.isfile(full_name):\n",
    "            continue\n",
    "        \n",
    "        with open(full_name, 'rb') as f:\n",
    "            # loop till the end of the file\n",
    "            while True:\n",
    "                # read only 1024 bytes at a time\n",
    "                chunk = f.read(read_chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                hashobj.update(chunk)\n",
    "        \n",
    "    return hashobj.hexdigest()\n",
    "\n",
    "def calc_hash_from_input_files(flow_object:dict, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files for a given flow object'''\n",
    "    return calc_hash_from_files(flow_object['in'], save_dir)\n",
    "\n",
    "def calc_hash_from_data_files(flow_object:dict, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files for a given flow object'''\n",
    "    return calc_hash_from_files(flow_object['in'] + flow_object['out'], save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'da4b2413f6a22c19a8a7823e6564e746'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_flowobject(data_file_flow[flow_object_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32095cd16a83a2c63f1ab51a58ed96c9'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_input_files(data_file_flow[flow_object_index], save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'65e6d36ac0c8aadb1fb4ee48d4ff88f3'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_data_files(data_file_flow[flow_object_index], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def check_script_inputs(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\" \n",
    "    Check if the input files for a script are up-to-date, returns True if up-to-date.\n",
    "    \"\"\"\n",
    "\n",
    "    checksum_file = get_save_path(f\"input-checksum-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    md5_check_result = subprocess.run(\n",
    "        ['md5sum', '-c', checksum_file], \n",
    "        cwd=save_dir,\n",
    "        capture_output=True)\n",
    "    \n",
    "    return int(md5_check_result.returncode) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_script_inputs(data_file_flow[0], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputs\n",
    "The output is easily checked for existence with `isfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def check_script_output(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\" \n",
    "    Check if the output files for a script exist, returns True if they all exist.\n",
    "    \"\"\"\n",
    "\n",
    "    return all([\n",
    "        os.path.isfile(get_save_path(F, save_dir)) \n",
    "        for F in flow_object['out']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_script_output(data_file_flow[0], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the checksum file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def generate_checksum_file(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\"Generates the checksum file for a given flow object\"\"\"\n",
    "\n",
    "    input_files = flow_object['in']\n",
    "    md5_encode_result = subprocess.run(\n",
    "        ['md5sum','-b']+\n",
    "        input_files, \n",
    "        cwd=save_dir,\n",
    "        capture_output=True)\n",
    "    \n",
    "    checksum_file = get_save_path(f\"input-checksum-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    with open(checksum_file, 'wt') as cf:\n",
    "        cf.write(md5_encode_result.stdout.decode('UTF-8'))\n",
    "\n",
    "    return md5_encode_result.returncode == 0 and check_script_inputs(flow_object, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_checksum_file(data_file_flow[0], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running once\n",
    "\n",
    "We don't need and don't _want't_ to run the script if more then once. This \n",
    "is not a problem when a script has finished and updated the checksum file, \n",
    "but we also want to prevent near simultaneous runs in a multiprocessing \n",
    "environment.\n",
    "\n",
    "We'll use file locking from fcntl directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use fcntl file locking to prevent multiple processes from running the same code at the same time.\n",
    "# see https://docs.python.org/3/library/fcntl.html#fcntl.flock\n",
    "\n",
    "# Create a filename based on input-file contents\n",
    "lock_file = get_save_path(f\"lock-{calc_hash_from_input_files(data_file_flow[0], save_dir)}\", save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `fcntl.flock` with flags `fcntl.LOCK_EX | fcntl.LOCK_NB` to lock the file for exclusive access, while\n",
    "an exception is thrown if it's already locked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 11] Resource temporarily unavailable\n"
     ]
    }
   ],
   "source": [
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "    with open(lock_file, 'wt') as cf2:\n",
    "        try:\n",
    "            fcntl.flock(cf2, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "        except BlockingIOError as locked_error:\n",
    "            print(locked_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locks are removed when the file is closed, how convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "### Synchroneous\n",
    "\n",
    "We need to run a script when either any of it's inputs have changed or any \n",
    "of it's outputs do not exist. Return True if a follow-up script should be \n",
    "executed, False if nothing changed or executing the script failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def run_rscript_wait(flow_object, assets_dir:str, save_dir:str):\n",
    "    \"\"\" Run a script in R \n",
    "        args:\n",
    "            flow_object: dict of flow object\n",
    "        returns:\n",
    "            bool: True if a follow-up script might need to be run, False if not\n",
    "\n",
    "    \"\"\"\n",
    "    syslog.debug(f\"Running script {flow_object['name']}\")\n",
    "    # Check if output exists and inputs have not changed and return False if \n",
    "    # output exists and inputs have not changed\n",
    "    if check_script_output(flow_object, save_dir) and check_script_inputs(flow_object, save_dir):\n",
    "        return True\n",
    "    \n",
    "    # Create the lock file\n",
    "    lock_file = get_save_path(f\"lock-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    with open(lock_file, 'wt') as cf:\n",
    "        try:\n",
    "            syslog.debug(f\"Locking {lock_file}\")\n",
    "            # Get exclusive lock on the file, is released on file close\n",
    "            fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "\n",
    "            # run the script\n",
    "            run_script_result = subprocess.run(\n",
    "                ['Rscript', '--vanilla', get_asset_path(flow_object['name'], assets_dir)],\n",
    "                cwd=save_dir,\n",
    "                capture_output=True\n",
    "            )\n",
    "            \n",
    "            # check the return code\n",
    "            if run_script_result.returncode:\n",
    "                cf.write(f\"Run returned code {run_script_result.returncode}\\n\")\n",
    "                cf.write('STDOUT------------\\n', run_script_result.stdout.decode('UTF-8'),'\\n')\n",
    "                cf.write('STDERR------------\\n', run_script_result.stderr.decode('UTF-8'),'\\n')\n",
    "                return False\n",
    "\n",
    "        except BlockingIOError as locked_error:\n",
    "            syslog.debug(locked_error)\n",
    "            return False\n",
    "\n",
    "    \n",
    "    # check the output and generate the checksum file\n",
    "    return check_script_output(flow_object, save_dir) and generate_checksum_file(flow_object, save_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Running script Data_preparation.R\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rscript_wait(data_file_flow[0], assets_dir, save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Running script Prediction_part1.R\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rscript_wait(data_file_flow[1], assets_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Running script Prediction_part2.R\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rscript_wait(data_file_flow[2], assets_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_results(flow_object :dict, save_dir:str):\n",
    "    \"\"\"Clear the results of a given flow object\"\"\"\n",
    "    for fname in flow_object['out']:\n",
    "        os.remove(get_save_path(fname, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_results(data_file_flow[0], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous\n",
    "\n",
    "In the API we can not wait for a script to finish and we'll use Popen instead. This\n",
    "means we'll have to keep track of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "RScriptProcess = namedtuple('RScriptProcess', ['lock_file', 'popen'])\n",
    "\n",
    "def run_rscript_nowait(flow_object, assets_dir:str, save_dir:str) -> RScriptProcess:\n",
    "    \"\"\" Run a script in R \n",
    "        args:\n",
    "            flow_object: dict of flow object\n",
    "            assets_dir: path to the assets directory\n",
    "            save_dir: path to the save directory\n",
    "        returns:\n",
    "            RScriptProcess: Popen container object for the script\n",
    "    \"\"\"\n",
    "    \n",
    "    syslog.debug(f\"Starting script {flow_object['name']}\")\n",
    "\n",
    "    # check lockfile ---------------------------------------------------------------\n",
    "    lock_name = calc_hash_from_flowobject(flow_object)\n",
    "    if run_rscript_nowait.lock_objects.get(lock_name): \n",
    "        lock_object = run_rscript_nowait.lock_objects[lock_name]\n",
    "        if not lock_object.lock_file.closed:\n",
    "            syslog.debug(f\"Lockfile is open for {flow_object['name']} ({lock_name})\")\n",
    "            \n",
    "            if lock_object.popen is not None:\n",
    "                syslog.debug(f\"No process running for {flow_object['name']} ({lock_name})\")\n",
    "            elif lock_object.popen.poll() is None:\n",
    "                syslog.debug(f\"Script is still running for {flow_object['name']} ({lock_name})\")\n",
    "                return lock_object\n",
    "            else:\n",
    "                syslog.debug(f\"Script has finished for {flow_object['name']} ({lock_name})\")\n",
    "                # since poll return not-None the script has finished so close the lockfile\n",
    "                lock_object.lock_file.close()\n",
    "\n",
    "\n",
    "    # Check if output exists and inputs have not changed and return False if \n",
    "    # output exists and inputs have not changed\n",
    "    if check_script_output(flow_object, save_dir) and check_script_inputs(flow_object, save_dir):\n",
    "        syslog.debug(f\"Output and inputs are up-to-date for {flow_object['name']}\")\n",
    "        return run_rscript_nowait.lock_objects.get(lock_name)\n",
    "\n",
    "    # Create the lock file -----------------------------------------------------------\n",
    "    syslog.debug(f\"Creating lockfile for {flow_object['name']} ({lock_name})\")\n",
    "    cf = open(get_save_path(f\"lock-{lock_name}\", save_dir), 'wt')\n",
    "    \n",
    "    try:\n",
    "        # Set lock on lockfile\n",
    "        fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "\n",
    "        # run the script\n",
    "        popen = subprocess.Popen(\n",
    "            ['Rscript', '--vanilla', get_asset_path(flow_object['name'], assets_dir)],\n",
    "            cwd=save_dir,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            encoding='UTF-8',\n",
    "        )\n",
    "\n",
    "        run_rscript_nowait.lock_objects[lock_name] =  RScriptProcess(cf, popen)\n",
    "            \n",
    "    except BlockingIOError as locked_error:\n",
    "        cf.close()\n",
    "        syslog.error(locked_error)\n",
    "\n",
    "    syslog.debug(f\"Done with {flow_object['name']}\")\n",
    "\n",
    "    return run_rscript_nowait.lock_objects.get(lock_name)\n",
    "\n",
    "run_rscript_nowait.lock_objects = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting script Data_preparation.R\n",
      "DEBUG:__main__:Creating lockfile for Data_preparation.R (da4b2413f6a22c19a8a7823e6564e746)\n",
      "DEBUG:__main__:Done with Data_preparation.R\n"
     ]
    }
   ],
   "source": [
    "run_result = run_rscript_nowait(data_file_flow[0], assets_dir, save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RScriptProcess(lock_file=<_io.TextIOWrapper name='/home/fenke/repos/corebridge/nbs/saves/rscript/lock-da4b2413f6a22c19a8a7823e6564e746' mode='wt' encoding='UTF-8'>, popen=<Popen: returncode: None args: ['Rscript', '--vanilla', '/home/fenke/repos/c...>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result.popen.poll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result.popen.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_result.popen.stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing lockfile /home/fenke/repos/corebridge/nbs/saves/rscript/lock-da4b2413f6a22c19a8a7823e6564e746\n"
     ]
    }
   ],
   "source": [
    "if run_result.popen.poll() is not None:\n",
    "    print(f\"Closing lockfile {run_result.lock_file.name}\")\n",
    "    run_result.lock_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting script Data_preparation.R\n",
      "DEBUG:__main__:Output and inputs are up-to-date for Data_preparation.R\n"
     ]
    }
   ],
   "source": [
    "test = run_rscript_nowait(data_file_flow[0], assets_dir, save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RScriptProcess(lock_file=<_io.TextIOWrapper name='/home/fenke/repos/corebridge/nbs/saves/rscript/lock-da4b2413f6a22c19a8a7823e6564e746' mode='wt' encoding='UTF-8'>, popen=<Popen: returncode: 0 args: ['Rscript', '--vanilla', '/home/fenke/repos/core...>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.popen.poll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AICore module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AICoreRModule(AICoreModule):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corebridge.venv",
   "language": "python",
   "name": "corebridge.venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
