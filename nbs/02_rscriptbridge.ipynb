{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RScript-Bridge\n",
    "\n",
    "> Bridge between Stactics AICore framework and RScript prediction scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some things to set up first\n",
    "\n",
    "Notebooks use nbdev thingses and `addroot` makes importing from\n",
    "the repo-directory more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import logging\n",
    "#import addroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corebridge.aicorebridge 0.2.27 from /home/fenke/repos/corebridge/corebridge/aicorebridge.py\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import os, logging, json, hashlib\n",
    "import fcntl, subprocess\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from corebridge.aicorebridge import AICoreModule\n",
    "from corebridge.core import init_console_logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AICore uses an `assets` dir from which we can read files, like scripts\n",
    "and a `save` dir were modules can write and read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "syslog = init_console_logging(__name__, logging.DEBUG, timestamp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assets_dir = os.path.join(os.path.abspath(os.getcwd()), 'assets', 'rscript')\n",
    "save_dir = os.path.join(os.path.abspath(os.getcwd()), 'saves', 'rscript')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def get_asset_path(script_name, assets_dir:str): \n",
    "    return os.path.join(assets_dir, script_name)\n",
    "def get_rscript_libpath(save_dir:str):\n",
    "    return os.path.join(save_dir, 'libs')\n",
    "def get_save_path(datafile_name:str, save_dir:str): \n",
    "    return os.path.join(save_dir, datafile_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def markdown_table(feature_dict):\n",
    "    return Markdown(tabulate(\n",
    "      [[v for v in row.values()] for row in feature_dict],\n",
    "      headers=[k for k in feature_dict[0].keys()],\n",
    "        tablefmt='github'\n",
    "    ))\n",
    "\n",
    "def markdown_flow_table(flow_table):\n",
    "    columns = set([C for S in flow_table for C in flow_table[S]])\n",
    "    print(columns)\n",
    "    return markdown_table()(\n",
    "        [\n",
    "            dict(file=fn, **{c:rd.get(c,' ') for c in columns}) \n",
    "            for fn, rd in flow_table.items()\n",
    "        ]    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def display_table(feature_dict):\n",
    "    display(markdown_table(feature_dict))\n",
    "    \n",
    "data_file_flow = {}\n",
    "\n",
    "def display_flow_table(flow_table):\n",
    "    columns = set([C for S in data_file_flow for C in data_file_flow[S]])\n",
    "    print(columns)\n",
    "    display(markdown_table(        [\n",
    "            dict(file=fn, **{c:rd.get(c,' ') for c in columns}) \n",
    "            for fn, rd in flow_table.items()\n",
    "        ]    \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running R code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts written in R can be run from a Python program using `subprocess` and `Rscript`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Rscript`\n",
    "\n",
    "A script can be run from the commandline with\n",
    "\n",
    "    Rscript ascript.R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `subproces`\n",
    "\n",
    "[Python's `subprocess`module](https://docs.python.org/3.11/library/subprocess.html#) has the tools to execute external programs like `Rscript`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1] \"hello world\"\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "subprocess.run(['Rscript',get_asset_path('hello.R', assets_dir)], capture_output=True).stdout.decode('UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: sapflow prediction scripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Data_preparation.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringer\n",
    "* zoo\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Data/Meta_data.csv`\n",
    "* `Data/Sapflux_Tilia_train.csv`\n",
    "* `Data/Weather_Tilia_train.csv`\n",
    "* `Data/Weather_Tilia_pred.csv`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Modelling_data.RData`\n",
    "* `Prediction_data.RData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Data_preparation.R'] = {\n",
    "      \"in\": [\n",
    "         \"Data/Meta_data.csv\",\n",
    "         \"Data/Sapflux_Tilia_train.csv\",\n",
    "         \"Data/Weather_Tilia_train.csv\",\n",
    "         \"Data/Weather_Tilia_pred.csv\"\n",
    "      ],\n",
    "      \"out\": [\n",
    "         \"Modelling_data.RData\",\n",
    "         \"Prediction_data.RData\"\n",
    "      ],\n",
    "    'libs':['lubridate', 'stringr', 'zoo']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part1.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "* mgcv\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Modelling_data.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Fitted_models.RData`\n",
    "* `Weights.RData`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part1.R'] = {\n",
    "      \"in\": [\n",
    "         \"Modelling_data.RData\"\n",
    "      ],\n",
    "      \"out\": [\n",
    "         \"Fitted_models.RData\",\n",
    "         \"Weights.RData\"\n",
    "      ],\n",
    "    'libs':['lubridate', 'stringr', 'mgcv']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part2.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "* mgcv\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Fitted_models.RData`\n",
    "* `Weights.RData`\n",
    "* `Modelling_data.RData`\n",
    "* `Prediction_data.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Predicted_sapflux.RData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part2.R'] = {\n",
    "    \"in\":[\n",
    "        'Fitted_models.RData',\n",
    "        'Weights.RData',\n",
    "        'Modelling_data.RData',\n",
    "        'Prediction_data.RData'\n",
    "    ],\n",
    "    \"out\":[\n",
    "        'Predicted_sapflux.RData'\n",
    "    ],\n",
    "    'libs':['lubridate', 'stringr', 'mgcv']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `Prediction_part3.R`\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "* lubridate\n",
    "* stringr\n",
    "\n",
    "#### Input\n",
    "\n",
    "* `Predicted_sapflux.RData`\n",
    "\n",
    "#### Output\n",
    "\n",
    "* `Predicted_water_usage.RData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_flow['Prediction_part3.R'] = {\n",
    "    'in':['Predicted_sapflux.RData'],\n",
    "    'out':['Predicted_water_usage.RData'],\n",
    "    'libs':['lubridate', 'stringr']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| data-file / script          | Data_preparation.R   | Prediction_part1.R   | Prediction_part2.R   | Prediction_part3.R   |\n",
       "|-----------------------------|----------------------|----------------------|----------------------|----------------------|\n",
       "| Meta_data.csv               | in                   | --                   | --                   | --                   |\n",
       "| Sapflux_Tilia_train.csv     | in                   | --                   | --                   | --                   |\n",
       "| Weather_Tilia_train.csv     | in                   | --                   | --                   | --                   |\n",
       "| Weather_Tilia_pred.csv      | in                   | --                   | --                   | --                   |\n",
       "| Modelling_data.RData        | out                  | in                   | in                   | --                   |\n",
       "| Prediction_data.RData       | out                  | --                   | in                   | --                   |\n",
       "| Fitted_models.RData         | --                   | out                  | in                   | --                   |\n",
       "| Weights.RData               | --                   | out                  | in                   | --                   |\n",
       "| Predicted_sapflux.RData     | --                   | --                   | out                  | in                   |\n",
       "| Predicted_water_usage.RData | --                   | --                   | --                   | out                  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "script_order = dict(zip(data_file_flow.keys(), range(len(data_file_flow.keys()))))\n",
    "\n",
    "# add the name to the objects\n",
    "data_file_flow = {\n",
    "    script_order[k]:{**v, 'name':k}\n",
    "    for k,v in data_file_flow.items()\n",
    "}\n",
    "\n",
    "\n",
    "data_files = reduce(\n",
    "    lambda Y,X:Y if (X in Y) else [*Y,X],\n",
    "    [\n",
    "        f\n",
    "        for S,P in data_file_flow.items() # patterns\n",
    "        for D,F in P.items()\n",
    "        for f in F\n",
    "        if D in ['in','out']\n",
    "        \n",
    "    ],\n",
    "    []\n",
    ")\n",
    "display(Markdown(tabulate(\n",
    "    [\n",
    "        [F.split('/')[-1]]+[\n",
    "            'in' if F in P['in'] else 'out' if F in P['out'] else '--' \n",
    "            for S,P in data_file_flow.items()\n",
    "        ] \n",
    "        for F in data_files\n",
    "    ],\n",
    "    headers=['data-file / script'] + [I['name'] for I in data_file_flow.values()],\n",
    "    tablefmt='github'\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/Meta_data.csv',\n",
       " 'Data/Sapflux_Tilia_train.csv',\n",
       " 'Data/Weather_Tilia_train.csv',\n",
       " 'Data/Weather_Tilia_pred.csv',\n",
       " 'Modelling_data.RData',\n",
       " 'Prediction_data.RData',\n",
       " 'Fitted_models.RData',\n",
       " 'Weights.RData',\n",
       " 'Predicted_sapflux.RData',\n",
       " 'Predicted_water_usage.RData']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'in': ['Data/Meta_data.csv',\n",
       "   'Data/Sapflux_Tilia_train.csv',\n",
       "   'Data/Weather_Tilia_train.csv',\n",
       "   'Data/Weather_Tilia_pred.csv'],\n",
       "  'out': ['Modelling_data.RData', 'Prediction_data.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'zoo'],\n",
       "  'name': 'Data_preparation.R'},\n",
       " 1: {'in': ['Modelling_data.RData'],\n",
       "  'out': ['Fitted_models.RData', 'Weights.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'mgcv'],\n",
       "  'name': 'Prediction_part1.R'},\n",
       " 2: {'in': ['Fitted_models.RData',\n",
       "   'Weights.RData',\n",
       "   'Modelling_data.RData',\n",
       "   'Prediction_data.RData'],\n",
       "  'out': ['Predicted_sapflux.RData'],\n",
       "  'libs': ['lubridate', 'stringr', 'mgcv'],\n",
       "  'name': 'Prediction_part2.R'},\n",
       " 3: {'in': ['Predicted_sapflux.RData'],\n",
       "  'out': ['Predicted_water_usage.RData'],\n",
       "  'libs': ['lubridate', 'stringr'],\n",
       "  'name': 'Prediction_part3.R'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import R libraries\n",
    "\n",
    "Importing libraries can be done with\n",
    "\n",
    "    Rscript -e 'install.packages(\"drat\", repos=\"https://cloud.r-project.org\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rscript (R) version 4.2.2 (2022-10-31)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subprocess.run(['Rscript','--version', ], capture_output=True).stdout.decode('UTF-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rscript (R) version 4.2.2 (2022-10-31)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rversion = subprocess.run(['Rscript','--version', ], capture_output=True)\n",
    "print(rversion.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User library folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_rscript_env(libfolder:str):\n",
    "    if os.environ.get('R_LIBS_USER'):\n",
    "        return dict(**os.environ)\n",
    "    else:\n",
    "        return dict(**os.environ, R_LIBS_USER=str(libfolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_rscript_env(get_rscript_libpath(save_dir)).get('R_LIBS_USER') == get_rscript_libpath(save_dir), 'rscript environment not set as expected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stringr', 'zoo', 'lubridate', 'mgcv']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([L for V in data_file_flow.values() for L in V['libs']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in library(lubridate) : there is no package called ‘lubridate’\n",
      "Execution halted\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "run_script_result = subprocess.run(['Rscript','-e', \"library(lubridate)\"], capture_output=True)\n",
    "print(run_script_result.stderr.decode('UTF-8'), run_script_result.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, False]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[os.path.exists(os.path.join(get_rscript_libpath(save_dir), L)) for L in list(set([L for V in data_file_flow.values() for L in V['libs']]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/fenke/repos/corebridge/nbs/saves/rscript/libs/stringr',\n",
       " '/home/fenke/repos/corebridge/nbs/saves/rscript/libs/zoo',\n",
       " '/home/fenke/repos/corebridge/nbs/saves/rscript/libs/lubridate',\n",
       " '/home/fenke/repos/corebridge/nbs/saves/rscript/libs/mgcv']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[os.path.join(get_rscript_libpath(save_dir), L) for L in list(set([L for V in data_file_flow.values() for L in V['libs']]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def check_rscript_libs(libs:list, libfolder:str):\n",
    "    \"\"\"Quick check if for all the R packages in libs a folder exists in libfolder\"\"\"\n",
    "    return all([os.path.exists(os.path.join(libfolder, L)) for L in libs])\n",
    "\n",
    "def check_rscript_lib(lib:str, libfolder:str) -> bool:\n",
    "    \"\"\"Checks if a R package is installed in libfolder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lib : str\n",
    "        name of the package\n",
    "    libfolder : str\n",
    "        path to the library folder\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the package is installed, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    run_script_result = subprocess.run(['Rscript','-e', f\"library({lib})\"], env=get_rscript_env(libfolder), capture_output=True)\n",
    "    if run_script_result.returncode != 0:\n",
    "        print('STDERR\\n', run_script_result.stderr.decode('UTF-8'))\n",
    "        print('STDOUT\\n', run_script_result.stdout.decode('UTF-8'))\n",
    "    return run_script_result.returncode == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_rscript_libs(list(set([L for V in data_file_flow.values() for L in V['libs']])), get_rscript_libpath(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_rscript_lib('mgcv', get_rscript_libpath(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_rscript_lib('zoo', get_rscript_libpath(save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def install_R_package_wait(pkg:str|list, workdir:str, repo='https://cloud.r-project.org'):\n",
    "    \"\"\"\n",
    "    Checks and if neccesary installs an R package\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pkg : str|list\n",
    "        name(s) of the package(s)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(pkg, str):\n",
    "        return install_R_package_wait([pkg], libfolder, repo)\n",
    "    \n",
    "    libfolder=os.path.join(workdir, 'libs')\n",
    "    os.makedirs(libfolder, exist_ok=True)\n",
    "    syslog.debug(f\"Using libfolder {libfolder} for packages\")\n",
    "    \n",
    "    env = dict(os.environ)\n",
    "    env['R_LIBS_USER'] = os.path.abspath(libfolder) \n",
    "    syslog.debug(F\"Using libfolder {env['R_LIBS_USER']} for R_LIBS_USER\")\n",
    "\n",
    "    \n",
    "    for pkg_i in pkg: # ['generics', 'timechange', 'rlang', 'stringi'] + \n",
    "        print(f\"\\nInstalling package {pkg_i}, testing attach ...\")\n",
    "        if not check_rscript_lib(pkg_i, libfolder):\n",
    "            print(f\"Package {pkg_i} not attached. Installing {pkg_i}\")\n",
    "            run_script_install = subprocess.run([\n",
    "                    'Rscript','-e', \n",
    "                    f\"install.packages('{pkg_i}', repos='{repo}', lib='{libfolder}', dependencies=TRUE)\"\n",
    "                ], capture_output=True, env=env)\n",
    "            \n",
    "            if run_script_install.returncode != 0:\n",
    "                print(f\"installing {pkg_i}, returned code {run_script_install.returncode} ... \")\n",
    "                print('STDOUT--------------\\n', run_script_install.stdout.decode('UTF-8'))\n",
    "                print('STDERR--------------\\n', run_script_install.stderr.decode('UTF-8'))\n",
    "\n",
    "            elif not check_rscript_lib(pkg_i, libfolder): # not in cache\n",
    "                print(f\"Attach after installing for {pkg_i} failed ... install logs below\")\n",
    "                print('STDOUT--------------\\n', run_script_install.stdout.decode('UTF-8'))\n",
    "                print('STDERR--------------\\n', run_script_install.stderr.decode('UTF-8'))\n",
    "            else:\n",
    "                print(f\"Attach after installation was successful. Library {pkg_i} appears to have been installed\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Attach successful. Library {pkg_i} appears to have been installed\")\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG\t28210\troot\t1929856308.py\t18\tUsing libfolder /home/fenke/repos/corebridge/nbs/saves/rscript/libs for packages\n",
      "DEBUG\t28210\troot\t1929856308.py\t22\tUsing libfolder /home/fenke/repos/corebridge/nbs/saves/rscript/libs for R_LIBS_USER\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing package generics, testing attach ...\n",
      "Attach successful. Library generics appears to have been installed\n",
      "\n",
      "Installing package timechange, testing attach ...\n",
      "Attach successful. Library timechange appears to have been installed\n",
      "\n",
      "Installing package rlang, testing attach ...\n",
      "Attach successful. Library rlang appears to have been installed\n"
     ]
    }
   ],
   "source": [
    "install_R_package_wait(['generics', 'timechange', 'rlang'], save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install_R_package_wait(\n",
    "#     ['zoo'],\n",
    "#     libfolder=get_rscript_libpath(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install_R_package_wait(\n",
    "#     sorted(list(set([L for V in data_file_flow.values() for L in V['libs']]))),\n",
    "#     libfolder=get_rscript_libpath(save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def unpack_assets(assets_dir:str, save_dir:str):\n",
    "    \"\"\"\n",
    "    Unpack the assets folder to the save_dir\n",
    "    \"\"\"\n",
    "    unpack_result = subprocess.Popen(\n",
    "        ['unzip', '-o', '-d', save_dir, os.path.join(assets_dir, '*.zip')],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    return unpack_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_result = unpack_assets(assets_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unzip',\n",
       " '-o',\n",
       " '-d',\n",
       " '/home/fenke/repos/corebridge/nbs/saves/rscript',\n",
       " '/home/fenke/repos/corebridge/nbs/assets/rscript/*.zip']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_result.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_result.poll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /home/fenke/repos/corebridge/nbs/assets/rscript/SapflowPredictionAssets.zip\n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Weather_Tilia_train.RData  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Weather_Tilia_train.csv  \n",
      " extracting: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Weather_Tilia_pred.RData  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Weather_Tilia_pred.csv  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Sapflux_Tilia_train.RData  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Sapflux_Tilia_train.csv  \n",
      " extracting: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Meta_data.RData  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Meta_data.csv  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Data/Historical_data.RData  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Prediction_part3.R  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Prediction_part2.R  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Prediction_part1.R  \n",
      "  inflating: /home/fenke/repos/corebridge/nbs/saves/rscript/Data_preparation.R  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(unpack_result.stdout.read().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checksum calculation\n",
    "\n",
    "Each script has it's own set of input files and should be run to\n",
    "update it's output when either it's inputs have changed or it's \n",
    "expected output does not exist.\n",
    "\n",
    "We can check for filechanges using a hashing algorithm, for \n",
    "instance MD5 or SHA-256. These are available either in Python\n",
    "or from the commandline.\n",
    "\n",
    "Lets look at the commandline version of MD5, on linux this is\n",
    "`md5sum`, with the input file for the preparation stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   \"Data/Meta_data.csv\",\n",
      "   \"Data/Sapflux_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_pred.csv\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data_file_flow[list(data_file_flow.keys())[0]]['in'], indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "md5sum will output hashes to stdout, which `subprocess.run` captures for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   \"Data/Meta_data.csv\",\n",
      "   \"Data/Sapflux_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_train.csv\",\n",
      "   \"Data/Weather_Tilia_pred.csv\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "flow_object_index = 0\n",
    "input_files = data_file_flow[flow_object_index]['in']\n",
    "\n",
    "print(json.dumps(input_files, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4bed61a77505bfd52032591d5c3a6050 *Data/Meta_data.csv\n",
      "6d705d98caa6618a4a990c3742c16564 *Data/Sapflux_Tilia_train.csv\n",
      "1232592f9488ce4fbb4ae11ba5be0349 *Data/Weather_Tilia_train.csv\n",
      "366dac1bf64003d1d08fca6121c036bd *Data/Weather_Tilia_pred.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "md5_encode_result = subprocess.run(\n",
    "    ['md5sum','-b']+\n",
    "    input_files, \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_encode_result.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to check the files we run it with the `-c` option and a file with the previously calculated checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = data_file_flow[flow_object_index]['name']\n",
    "\n",
    "checksum_file = get_save_path(f\"input-checksum-{script_name.split('.')[0]}\", save_dir)\n",
    "with open(checksum_file, 'wt') as cf:\n",
    "    cf.write(md5_encode_result.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Meta_data.csv: OK\n",
      "Data/Sapflux_Tilia_train.csv: OK\n",
      "Data/Weather_Tilia_train.csv: OK\n",
      "Data/Weather_Tilia_pred.csv: OK\n",
      "\n",
      "Run returned code 0\n"
     ]
    }
   ],
   "source": [
    "md5_check_result = subprocess.run(\n",
    "    ['md5sum', '-c', checksum_file], \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_check_result.stdout.decode('UTF-8'))\n",
    "print(f\"Run returned code {md5_check_result.returncode}\")\n",
    "if md5_check_result.returncode:\n",
    "    print(md5_check_result.stderr.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had there been a change to a file it would have looked like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run returned code 1\n"
     ]
    }
   ],
   "source": [
    "md5_check_result = subprocess.run(\n",
    "    ['md5sum', '-c', checksum_file+'-modified'], \n",
    "    cwd=save_dir,\n",
    "    capture_output=True)\n",
    "print(md5_check_result.stdout.decode('UTF-8'))\n",
    "print(f\"Run returned code {md5_check_result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't really need specifics, only the return code will\n",
    "do for our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking files\n",
    "\n",
    "\n",
    "#### Generating names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "read_chunk_size = 1024 * 32\n",
    "def calc_hash_from_flowobject(flow_object:dict)->str:\n",
    "    '''Calculate a unique hash for a given flow object'''\n",
    "    return hashlib.md5(repr(flow_object).encode('UTF-8')).hexdigest()\n",
    "\n",
    "def calc_hash_from_files(files:list, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files'''\n",
    "    hashobj = hashlib.md5()\n",
    "\n",
    "    # iterate over files \n",
    "    for data_file in files:\n",
    "        full_name = os.path.join(save_dir, data_file)\n",
    "        if not os.path.isfile(full_name):\n",
    "            continue\n",
    "        \n",
    "        with open(full_name, 'rb') as f:\n",
    "            # loop till the end of the file\n",
    "            while True:\n",
    "                # read only 1024 bytes at a time\n",
    "                chunk = f.read(read_chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                hashobj.update(chunk)\n",
    "        \n",
    "    return hashobj.hexdigest()\n",
    "\n",
    "def calc_hash_from_input_files(flow_object:dict, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files for a given flow object'''\n",
    "    return calc_hash_from_files(flow_object['in'], save_dir)\n",
    "\n",
    "def calc_hash_from_data_files(flow_object:dict, save_dir:str)->str:\n",
    "    '''Calculate hash from the contents of the input files for a given flow object'''\n",
    "    return calc_hash_from_files(flow_object['in'] + flow_object['out'], save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'da4b2413f6a22c19a8a7823e6564e746'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_flowobject(data_file_flow[flow_object_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32095cd16a83a2c63f1ab51a58ed96c9'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_input_files(data_file_flow[flow_object_index], save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'65e6d36ac0c8aadb1fb4ee48d4ff88f3'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hash_from_data_files(data_file_flow[flow_object_index], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def check_script_inputs(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\" \n",
    "    Check if the input files for a script are up-to-date, returns True if up-to-date.\n",
    "    \"\"\"\n",
    "\n",
    "    checksum_file = get_save_path(f\"input-checksum-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    checksum_file = get_save_path(f\"input-checksum-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    md5_check_result = subprocess.run(\n",
    "        ['md5sum', '-c', checksum_file], \n",
    "        cwd=save_dir,\n",
    "        capture_output=True)\n",
    "    syslog.debug(f\"Checksum check result for Flow object: {flow_object['name']}: {md5_check_result.returncode}, checksum file: {checksum_file}\")\n",
    "    \n",
    "    return int(md5_check_result.returncode) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_script_inputs(data_file_flow[1], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputs\n",
    "The output is easily checked for existence with `isfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def check_script_output(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\" \n",
    "    Check if the output files for a script exist, returns True if they all exist.\n",
    "    \"\"\"\n",
    "\n",
    "    return all([\n",
    "        os.path.isfile(get_save_path(F, save_dir)) \n",
    "        for F in flow_object['out']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_script_output(data_file_flow[1], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the checksum file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def generate_checksum_file(flow_object:dict, save_dir:str)->bool:\n",
    "    \"\"\"Generates the checksum file for a given flow object\"\"\"\n",
    "\n",
    "    input_files = flow_object['in']\n",
    "    md5_encode_result = subprocess.run(\n",
    "        ['md5sum','-b']+\n",
    "        input_files, \n",
    "        cwd=save_dir,\n",
    "        capture_output=True)\n",
    "    \n",
    "    checksum_file = get_save_path(f\"input-checksum-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    syslog.debug(f\"Checksum file for Flow object: {flow_object['name']} created return {md5_encode_result.returncode}, checksum file: {checksum_file}\")\n",
    "    with open(checksum_file, 'wt') as cf:\n",
    "        cf.write(md5_encode_result.stdout.decode('UTF-8'))\n",
    "\n",
    "    return md5_encode_result.returncode == 0 and check_script_inputs(flow_object, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG\t28210\troot\t2564925326.py\t14\tFlow object: Data_preparation.R, checksum file: /home/fenke/repos/corebridge/nbs/saves/rscript/input-checksum-da4b2413f6a22c19a8a7823e6564e746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_checksum_file(data_file_flow[0], save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running once\n",
    "\n",
    "We don't need and don't _want't_ to run the script if more then once. This \n",
    "is not a problem when a script has finished and updated the checksum file, \n",
    "but we also want to prevent near simultaneous runs in a multiprocessing \n",
    "environment.\n",
    "\n",
    "We'll use file locking from fcntl directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use fcntl file locking to prevent multiple processes from running the same code at the same time.\n",
    "# see https://docs.python.org/3/library/fcntl.html#fcntl.flock\n",
    "\n",
    "# Create a filename based on input-file contents\n",
    "lock_file = get_save_path(f\"lock-{calc_hash_from_input_files(data_file_flow[0], save_dir)}\", save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `fcntl.flock` with flags `fcntl.LOCK_EX | fcntl.LOCK_NB` to lock the file for exclusive access, while\n",
    "an exception is thrown if it's already locked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 11] Resource temporarily unavailable\n"
     ]
    }
   ],
   "source": [
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "    with open(lock_file, 'wt') as cf2:\n",
    "        try:\n",
    "            fcntl.flock(cf2, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "        except BlockingIOError as locked_error:\n",
    "            print(locked_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locks are removed when the file is closed, how convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "with open(lock_file, 'wt') as cf:\n",
    "    fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "### Synchroneous\n",
    "\n",
    "We need to run a script when either any of it's inputs have changed or any \n",
    "of it's outputs do not exist. Return True if a follow-up script should be \n",
    "executed, False if nothing changed or executing the script failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def run_rscript_wait(flow_object, assets_dir:str, save_dir:str):\n",
    "    \"\"\" Run a script in R \n",
    "        args:\n",
    "            flow_object: dict of flow object\n",
    "        returns:\n",
    "            bool: True if a follow-up script might need to be run, False if not\n",
    "\n",
    "    \"\"\"\n",
    "    syslog.debug(f\"Running script {flow_object['name']}\")\n",
    "    # Check if output exists and inputs have not changed and return False if \n",
    "    # output exists and inputs have not changed\n",
    "    if check_script_output(flow_object, save_dir) and check_script_inputs(flow_object, save_dir):\n",
    "        return True\n",
    "    \n",
    "    # Create the lock file\n",
    "    lock_file = get_save_path(f\"lock-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "    with open(lock_file, 'wt') as cf:\n",
    "        try:\n",
    "            syslog.debug(f\"Locking {lock_file}\")\n",
    "            # Get exclusive lock on the file, is released on file close\n",
    "            fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "\n",
    "            # run the script\n",
    "            run_script_result = subprocess.run(\n",
    "                ['Rscript', '--vanilla', get_asset_path(flow_object['name'], assets_dir)],\n",
    "                cwd=save_dir,\n",
    "                capture_output=True\n",
    "            )\n",
    "            \n",
    "            # check the return code\n",
    "            if run_script_result.returncode:\n",
    "                cf.write(f\"Run returned code {run_script_result.returncode}\\n\")\n",
    "                cf.write(f\"STDOUT------------\\n{run_script_result.stdout.decode('UTF-8')}\\n\")\n",
    "                cf.write(f\"STDERR------------\\n{run_script_result.stderr.decode('UTF-8')}\\n\")\n",
    "                return False\n",
    "\n",
    "        except BlockingIOError as locked_error:\n",
    "            syslog.debug(locked_error)\n",
    "            return False\n",
    "\n",
    "    \n",
    "    # check the output and generate the checksum file\n",
    "    return check_script_output(flow_object, save_dir) and generate_checksum_file(flow_object, save_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rscript_wait(data_file_flow[0], assets_dir, save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rscript_wait(data_file_flow[1], assets_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rscript_wait(data_file_flow[2], assets_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_results(flow_object :dict, save_dir:str):\n",
    "    \"\"\"Clear the results of a given flow object\"\"\"\n",
    "    for fname in flow_object['out']:\n",
    "        try:\n",
    "            os.remove(get_save_path(fname, save_dir))\n",
    "        except FileNotFoundError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clear_results(O, save_dir) for O in data_file_flow.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous\n",
    "\n",
    "In the API we can not wait for a script to finish and we'll use Popen instead. This\n",
    "means we'll have to keep track of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "RScriptProcess = namedtuple('RScriptProcess', ['flow_object', 'lock_file', 'stdout','stderr', 'popen_args', 'popen'])\n",
    "\n",
    "#### Asynchronous RScript processing ------------------------------------------------\n",
    "\n",
    "def run_rscript_nowait(flow_object, workdir:str, pkg_repo:str='https://cloud.r-project.org') -> RScriptProcess:\n",
    "    \"\"\" Run a script in R \n",
    "        args:\n",
    "            flow_object: dict of flow object\n",
    "            workdir: working directory\n",
    "            pkg_repo: CRAN package repository\n",
    "        returns:\n",
    "            RScriptProcess: Popen container object for the script\n",
    "    \"\"\"\n",
    "    \n",
    "    syslog.debug(f\"Starting rscript for {flow_object['name']}\")\n",
    "\n",
    "    # lockfile -------------------------------------------------------------------\n",
    "    os.makedirs(os.path.join(workdir, 'temp'), exist_ok=True)\n",
    "    def get_temp_path(lname):\n",
    "        return os.path.join(workdir, 'temp', lname)\n",
    "    \n",
    "    lock_name = 'run_flow_object-'+calc_hash_from_flowobject(flow_object)\n",
    "\n",
    "    # lock maintenance\n",
    "    if run_rscript_nowait.lock_objects.get(lock_name): \n",
    "        lock_object = run_rscript_nowait.lock_objects[lock_name]\n",
    "        if not lock_object.lock_file.closed:\n",
    "            syslog.debug(f\"Lockfile is open for {flow_object['name']} ({lock_name})\")\n",
    "            # If the lockfile is open, check if the process is still running\n",
    "            \n",
    "            if lock_object.popen is None:\n",
    "                syslog.debug(f\"No process running for {flow_object['name']} ({lock_name})\")\n",
    "            elif lock_object.popen.poll() is None:\n",
    "                syslog.debug(f\"Script is still running for {flow_object['name']} ({lock_name})\")\n",
    "                return lock_object\n",
    "            else:\n",
    "                syslog.debug(f\"Script has finished for {flow_object['name']} ({lock_name}), returned {lock_object.popen.returncode}\")\n",
    "                # since poll return not-None the script has finished so close the lockfile\n",
    "                lock_object.lock_file.close()\n",
    "                lock_object.stdout.close()\n",
    "                lock_object.stderr.close()\n",
    "                if lock_object.popen.returncode != 0:\n",
    "                    syslog.error(f\"Script failed for {flow_object['name']} ({lock_name}), returned {lock_object.popen.returncode}\")\n",
    "                    syslog.error(f\"Args were: {lock_object.popen_args}\")\n",
    "                    with open(lock_object.stdout.name, 'rb') as so:\n",
    "                        syslog.error(f\"STDOUT\\n{so.read().decode('UTF-8')}\")\n",
    "                    with open(lock_object.stderr.name, 'rb') as se:\n",
    "                        syslog.error(f\"STDERR\\n{se.read().decode('UTF-8')}\")\n",
    "                else:\n",
    "                    syslog.debug(f\"Script was successful for {flow_object['name']} ({lock_name})\")\n",
    "                    generate_checksum_file(flow_object, workdir)\n",
    "\n",
    "                #os.remove(lock_object.stdout.name)\n",
    "                #os.remove(lock_object.stderr.name)\n",
    "\n",
    "\n",
    "    # Check if output exists and inputs have not changed and return False if \n",
    "    # output exists and inputs have not changed\n",
    "    if check_script_output(flow_object, workdir) and check_script_inputs(flow_object, workdir):\n",
    "        syslog.debug(f\"Output and inputs are up-to-date for {flow_object['name']}\")\n",
    "        return run_rscript_nowait.lock_objects.get(lock_name)\n",
    "\n",
    "    # Create the lock file -----------------------------------------------------------\n",
    "    syslog.debug(f\"Preparing to run scripts for {flow_object['name']}, creating lockfile ({lock_name})\")\n",
    "    cf = open(get_temp_path(f\"lock-{lock_name}\"), 'wt')\n",
    "    \n",
    "    try:\n",
    "        # Set lock on lockfile\n",
    "        fcntl.flock(cf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "\n",
    "        so = open(get_temp_path(f\"stdout-{lock_name}\"), 'wt')\n",
    "        se = open(get_temp_path(f\"stderr-{lock_name}\"), 'wt')\n",
    "\n",
    "        # check libs\n",
    "        libfolder=os.path.join(workdir, 'libs')\n",
    "        os.makedirs(libfolder, exist_ok=True)\n",
    "        syslog.debug(f\"Using libfolder {libfolder} for packages\")\n",
    "        \n",
    "        env = dict(os.environ)\n",
    "        env['R_LIBS_USER'] = os.path.abspath(libfolder) \n",
    "        syslog.debug(F\"Using libfolder {env['R_LIBS_USER']} for R_LIBS_USER\")\n",
    "        \n",
    "        if not check_rscript_libs(flow_object['libs'], libfolder):\n",
    "            for pkg_i in flow_object['libs']:\n",
    "                syslog.debug(f\"Checking lib {pkg_i} for {flow_object['name']} ({lock_name})\")\n",
    "                if not check_rscript_lib(pkg_i, libfolder):\n",
    "                    syslog.debug(f\"Starting installation of {pkg_i} for {flow_object['name']} ({lock_name})\")\n",
    "                    popen_args = [\n",
    "                            'Rscript','-e', \n",
    "                            f\"install.packages('{pkg_i}', repos='{pkg_repo}', lib='{libfolder}', dependencies=TRUE)\",\n",
    "                        ]\n",
    "                    run_script_install = subprocess.Popen(\n",
    "                        popen_args, \n",
    "                        cwd=workdir,\n",
    "                        stdout=so,\n",
    "                        stderr=se,\n",
    "                        encoding='UTF-8',\n",
    "                        env=env,\n",
    "                    )\n",
    "                    run_rscript_nowait.lock_objects[lock_name] =  RScriptProcess(flow_object, cf, so, se, popen_args, run_script_install)\n",
    "                    return run_rscript_nowait.lock_objects.get(lock_name)\n",
    "                    \n",
    "        \n",
    "        syslog.debug(f\"Libs are up-to-date, starting script for {flow_object['name']} ({lock_name})\")\n",
    "        # run the script\n",
    "        popen_args = ['Rscript', flow_object['name']]\n",
    "        popen_run = subprocess.Popen(\n",
    "            popen_args,\n",
    "            cwd=workdir,\n",
    "            stdout=so,\n",
    "            stderr=se,\n",
    "            encoding='UTF-8',\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "        run_rscript_nowait.lock_objects[lock_name] =  RScriptProcess(flow_object, cf, so, se, popen_args, popen_run)\n",
    "            \n",
    "    except BlockingIOError as locked_error:\n",
    "        cf.close()\n",
    "        #syslog.error(f\"{flow_object['name']} is locked, cannot run\", exc_info=locked_error)\n",
    "\n",
    "    syslog.debug(f\"Done with {flow_object['name']}.\")\n",
    "\n",
    "    return run_rscript_nowait.lock_objects.get(lock_name)\n",
    "\n",
    "run_rscript_nowait.lock_objects = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\t28210\troot\t2844065539.py\t2\tData_preparation.R --------------------\n",
      "DEBUG\t28210\troot\t503119086.py\t17\tStarting rscript for Data_preparation.R\n",
      "DEBUG\t28210\troot\t2579484379.py\t13\tChecksum check result for Flow object: Data_preparation.R: 0, checksum file: /home/fenke/repos/corebridge/nbs/saves/rscript/input-checksum-da4b2413f6a22c19a8a7823e6564e746\n",
      "DEBUG\t28210\troot\t503119086.py\t62\tOutput and inputs are up-to-date for Data_preparation.R\n",
      "INFO\t28210\troot\t2844065539.py\t2\tPrediction_part1.R --------------------\n",
      "DEBUG\t28210\troot\t503119086.py\t17\tStarting rscript for Prediction_part1.R\n",
      "DEBUG\t28210\troot\t2579484379.py\t13\tChecksum check result for Flow object: Prediction_part1.R: 0, checksum file: /home/fenke/repos/corebridge/nbs/saves/rscript/input-checksum-6ea0e8b3895468772af891ee7b90a11e\n",
      "DEBUG\t28210\troot\t503119086.py\t62\tOutput and inputs are up-to-date for Prediction_part1.R\n",
      "INFO\t28210\troot\t2844065539.py\t2\tPrediction_part2.R --------------------\n",
      "DEBUG\t28210\troot\t503119086.py\t17\tStarting rscript for Prediction_part2.R\n",
      "DEBUG\t28210\troot\t2579484379.py\t13\tChecksum check result for Flow object: Prediction_part2.R: 0, checksum file: /home/fenke/repos/corebridge/nbs/saves/rscript/input-checksum-70dfaa2a3c99ed6ac2c53df7e144b4d8\n",
      "DEBUG\t28210\troot\t503119086.py\t62\tOutput and inputs are up-to-date for Prediction_part2.R\n",
      "INFO\t28210\troot\t2844065539.py\t2\tPrediction_part3.R --------------------\n",
      "DEBUG\t28210\troot\t503119086.py\t17\tStarting rscript for Prediction_part3.R\n",
      "DEBUG\t28210\troot\t2579484379.py\t13\tChecksum check result for Flow object: Prediction_part3.R: 0, checksum file: /home/fenke/repos/corebridge/nbs/saves/rscript/input-checksum-0ae0a2f9f9ce7fe6231402e57d595772\n",
      "DEBUG\t28210\troot\t503119086.py\t62\tOutput and inputs are up-to-date for Prediction_part3.R\n"
     ]
    }
   ],
   "source": [
    "for flow_object in data_file_flow.values():\n",
    "    syslog.info(f\"{flow_object['name']} --------------------\")\n",
    "    startresult = run_rscript_nowait(flow_object, workdir=save_dir)\n",
    "    \n",
    "    #print(f\"Args: {startresult.popen_args if startresult else None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process run_flow_object-6ea0e8b3895468772af891ee7b90a11e for Prediction_part1.R is done? 0\n",
      "args: ['Rscript', 'Prediction_part1.R']\n",
      "process run_flow_object-0ae0a2f9f9ce7fe6231402e57d595772 for Prediction_part3.R is done? 0\n",
      "args: ['Rscript', 'Prediction_part3.R']\n",
      "process run_flow_object-70dfaa2a3c99ed6ac2c53df7e144b4d8 for Prediction_part2.R is done? 0\n",
      "args: ['Rscript', 'Prediction_part2.R']\n"
     ]
    }
   ],
   "source": [
    "for name, process in run_rscript_nowait.lock_objects.items():\n",
    "    if process.popen:\n",
    "        print(f\"process {name} for {process.flow_object['name']} is done? {process.popen.poll()}\")\n",
    "        print(f\"args: {process.popen_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\t28210\troot\t1138821752.py\t2\tChecking Data_preparation.R\n",
      "INFO\t28210\troot\t1138821752.py\t4\tOutput and inputs are up-to-date for Data_preparation.R\n",
      "INFO\t28210\troot\t1138821752.py\t2\tChecking Prediction_part1.R\n",
      "INFO\t28210\troot\t1138821752.py\t2\tChecking Prediction_part2.R\n",
      "INFO\t28210\troot\t1138821752.py\t2\tChecking Prediction_part3.R\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fitted_models.RData', True), ('Weights.RData', True)]\n",
      "1 md5sum: /home/fenke/repos/corebridge/nbs/saves/rscript/input-checksum-6ea0e8b3895468772af891ee7b90a11e: No such file or directory\n",
      "\n",
      "[('Predicted_sapflux.RData', True)]\n",
      "1 md5sum: /home/fenke/repos/corebridge/nbs/saves/rscript/input-checksum-70dfaa2a3c99ed6ac2c53df7e144b4d8: No such file or directory\n",
      "\n",
      "[('Predicted_water_usage.RData', True)]\n",
      "1 md5sum: /home/fenke/repos/corebridge/nbs/saves/rscript/input-checksum-0ae0a2f9f9ce7fe6231402e57d595772: No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for flow_object in data_file_flow.values():\n",
    "    syslog.info(f\"Checking {flow_object['name']}\")\n",
    "    if check_script_output(flow_object, save_dir) and check_script_inputs(flow_object, save_dir):\n",
    "        syslog.info(f\"Output and inputs are up-to-date for {flow_object['name']}\")\n",
    "    else:\n",
    "        print([\n",
    "            (F,os.path.isfile(get_save_path(F, save_dir)) )\n",
    "            for F in flow_object['out']\n",
    "        ])\n",
    "        checksum_file = get_save_path(f\"input-checksum-{calc_hash_from_flowobject(flow_object)}\", save_dir)\n",
    "        md5_check_result = subprocess.run(\n",
    "            ['md5sum', '-c', checksum_file], \n",
    "            cwd=save_dir,\n",
    "            capture_output=True)\n",
    "        \n",
    "        print(md5_check_result.returncode, md5_check_result.stderr.decode('UTF-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing lockfile /home/fenke/repos/corebridge/nbs/saves/rscript/temp/lock-run_flow_object-da4b2413f6a22c19a8a7823e6564e746\n"
     ]
    }
   ],
   "source": [
    "for process in run_rscript_nowait.lock_objects.values():\n",
    "    if process.popen and process.popen.poll() is not None:\n",
    "        print(f\"Closing lockfile {process.lock_file.name}\")\n",
    "        process.lock_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AICore module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AICoreRModule(AICoreModule):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corebridge.venv",
   "language": "python",
   "name": "corebridge.venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
